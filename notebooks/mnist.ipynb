{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from control_variates.model import MLP\n",
    "from control_variates.optim import LangevinSGD as SGLD, ScaleAdaSGHMC as H_SA_SGHMC\n",
    "from mnist_utils import load_mnist_dataset\n",
    "from control_variates.trainer import BNNTrainer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from pathlib import Path\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "input_dim = 784\n",
    "width = 100\n",
    "depth = 2\n",
    "output_dim = 10\n",
    "lr = 1e-3\n",
    "n_epoch = 300\n",
    "alpha0, beta0 = 10, 10\n",
    "resample_prior_every = 15\n",
    "resample_momentum_every = 50\n",
    "burn_in_epochs = 20\n",
    "save_freq = 2\n",
    "resample_prior_until = 100\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Path('../data', 'mnist').mkdir(exist_ok=True, parents=True)\n",
    "trainloader, valloader = load_mnist_dataset(Path('../data', 'mnist'), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGLD(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "optimizer = H_SA_SGHMC(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_func(y_hat, y):\n",
    "    nll = F.cross_entropy(y_hat, y, reduction='sum')\n",
    "    return nll\n",
    "\n",
    "def err_func(y_hat, y):\n",
    "    err = y_hat.argmax(-1).ne(y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BNNTrainer(model, optimizer, nll_func, err_func, trainloader, valloader, device=device, \n",
    "    resample_prior_every=resample_prior_every,\n",
    "    resample_momentum_every=resample_momentum_every,\n",
    "    save_freq=save_freq,\n",
    "    batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": " loss 0.06701449304819107, Val error 0.0199\n2020-08-14 12:53:41,426 Epoch 80 finished. Val loss 0.07195600867271423, Val error 0.0225\n2020-08-14 12:53:49,692 Epoch 81 finished. Val loss 0.07047393172979355, Val error 0.0203\n2020-08-14 12:53:58,345 Epoch 82 finished. Val loss 0.0689258798956871, Val error 0.0214\n2020-08-14 12:54:07,573 Epoch 83 finished. Val loss 0.07056793570518494, Val error 0.0219\n2020-08-14 12:54:16,372 Epoch 84 finished. Val loss 0.06595133990049362, Val error 0.0207\n2020-08-14 12:54:27,838 Epoch 85 finished. Val loss 0.0649980679154396, Val error 0.0202\n2020-08-14 12:54:37,796 Epoch 86 finished. Val loss 0.07060126960277557, Val error 0.0218\n2020-08-14 12:54:49,623 Epoch 87 finished. Val loss 0.0725543275475502, Val error 0.0226\n2020-08-14 12:54:58,163 Epoch 88 finished. Val loss 0.05860758200287819, Val error 0.0185\n2020-08-14 12:55:06,278 Epoch 89 finished. Val loss 0.06630147993564606, Val error 0.0204\n2020-08-14 12:55:14,271 Epoch 90 finished. Val loss 0.06601616740226746, Val error 0.0196\n2020-08-14 12:55:22,381 Epoch 91 finished. Val loss 0.07027024030685425, Val error 0.0216\n2020-08-14 12:55:30,307 Epoch 92 finished. Val loss 0.06825648248195648, Val error 0.0217\n2020-08-14 12:55:39,623 Epoch 93 finished. Val loss 0.06944569945335388, Val error 0.0225\n2020-08-14 12:55:49,736 Epoch 94 finished. Val loss 0.06596875190734863, Val error 0.0198\n2020-08-14 12:55:58,875 Epoch 95 finished. Val loss 0.06656596064567566, Val error 0.02\n2020-08-14 12:56:08,337 Epoch 96 finished. Val loss 0.07258154451847076, Val error 0.0226\n2020-08-14 12:56:19,103 Epoch 97 finished. Val loss 0.07665721327066422, Val error 0.024\n2020-08-14 12:56:28,756 Epoch 98 finished. Val loss 0.06241890415549278, Val error 0.0193\n2020-08-14 12:56:37,691 Epoch 99 finished. Val loss 0.06611023843288422, Val error 0.0205\n2020-08-14 12:56:47,009 Epoch 100 finished. Val loss 0.0686938688158989, Val error 0.0212\n2020-08-14 12:56:54,936 Epoch 101 finished. Val loss 0.06812462955713272, Val error 0.0216\n2020-08-14 12:57:03,107 Epoch 102 finished. Val loss 0.06681443005800247, Val error 0.0208\n2020-08-14 12:57:13,876 Epoch 103 finished. Val loss 0.06180604174733162, Val error 0.0181\n2020-08-14 12:57:23,484 Epoch 104 finished. Val loss 0.06892265379428864, Val error 0.0207\n2020-08-14 12:57:31,507 Epoch 105 finished. Val loss 0.0594828724861145, Val error 0.0168\n2020-08-14 12:57:40,300 Epoch 106 finished. Val loss 0.07314226031303406, Val error 0.0227\n2020-08-14 12:57:48,785 Epoch 107 finished. Val loss 0.06853685528039932, Val error 0.0203\n2020-08-14 12:57:59,291 Epoch 108 finished. Val loss 0.06422159075737, Val error 0.0184\n2020-08-14 12:58:08,783 Epoch 109 finished. Val loss 0.0620708242058754, Val error 0.0192\n2020-08-14 12:58:19,525 Epoch 110 finished. Val loss 0.0631440207362175, Val error 0.0199\n2020-08-14 12:58:29,778 Epoch 111 finished. Val loss 0.06106937304139137, Val error 0.0179\n2020-08-14 12:58:39,439 Epoch 112 finished. Val loss 0.07099727541208267, Val error 0.0209\n2020-08-14 12:58:48,437 Epoch 113 finished. Val loss 0.06403850764036179, Val error 0.0212\n2020-08-14 12:58:57,152 Epoch 114 finished. Val loss 0.07271837443113327, Val error 0.0215\n2020-08-14 12:59:07,650 Epoch 115 finished. Val loss 0.06390925496816635, Val error 0.0194\n2020-08-14 12:59:18,186 Epoch 116 finished. Val loss 0.0690336599946022, Val error 0.0221\n2020-08-14 12:59:30,284 Epoch 117 finished. Val loss 0.06347627192735672, Val error 0.0209\n2020-08-14 12:59:41,845 Epoch 118 finished. Val loss 0.06614278256893158, Val error 0.0208\n2020-08-14 12:59:52,343 Epoch 119 finished. Val loss 0.06783705949783325, Val error 0.0208\n2020-08-14 13:00:01,947 Epoch 120 finished. Val loss 0.06772395223379135, Val error 0.0206\n2020-08-14 13:00:11,670 Epoch 121 finished. Val loss 0.06586994230747223, Val error 0.0216\n2020-08-14 13:00:20,547 Epoch 122 finished. Val loss 0.06233235448598862, Val error 0.0183\n2020-08-14 13:00:29,458 Epoch 123 finished. Val loss 0.06694565713405609, Val error 0.02\n2020-08-14 13:00:38,152 Epoch 124 finished. Val loss 0.07029296457767487, Val error 0.0216\n2020-08-14 13:00:46,812 Epoch 125 finished. Val loss 0.0649881362915039, Val error 0.0217\n2020-08-14 13:00:58,140 Epoch 126 finished. Val loss 0.07536768913269043, Val error 0.0234\n2020-08-14 13:01:09,647 Epoch 127 finished. Val loss 0.06851137429475784, Val error 0.0217\n2020-08-14 13:01:18,286 Epoch 128 finished. Val loss 0.06368386745452881, Val error 0.0195\n2020-08-14 13:01:29,172 Epoch 129 finished. Val loss 0.0638662576675415, Val error 0.0204\n2020-08-14 13:01:38,829 Epoch 130 finished. Val loss 0.06909731775522232, Val error 0.0219\n2020-08-14 13:01:49,797 Epoch 131 finished. Val loss 0.06488369405269623, Val error 0.0196\n2020-08-14 13:02:02,263 Epoch 132 finished. Val loss 0.0638352632522583, Val error 0.0203\n2020-08-14 13:02:13,381 Epoch 133 finished. Val loss 0.06433234363794327, Val error 0.0187\n2020-08-14 13:02:22,953 Epoch 134 finished. Val loss 0.06675011664628983, Val error 0.02\n2020-08-14 13:02:30,958 Epoch 135 finished. Val loss 0.06441117823123932, Val error 0.0189\n2020-08-14 13:02:39,875 Epoch 136 finished. Val loss 0.0681566521525383, Val error 0.0217\n2020-08-14 13:02:49,352 Epoch 137 finished. Val loss 0.06189943104982376, Val error 0.0195\n2020-08-14 13:02:57,670 Epoch 138 finished. Val loss 0.06684070080518723, Val error 0.0211\n2020-08-14 13:03:08,135 Epoch 139 finished. Val loss 0.06143581494688988, Val error 0.0174\n2020-08-14 13:03:20,591 Epoch 140 finished. Val loss 0.06716150045394897, Val error 0.0209\n2020-08-14 13:03:31,828 Epoch 141 finished. Val loss 0.07549871504306793, Val error 0.0229\n2020-08-14 13:03:39,485 Epoch 142 finished. Val loss 0.07043377310037613, Val error 0.0224\n2020-08-14 13:03:47,179 Epoch 143 finished. Val loss 0.0646219328045845, Val error 0.0201\n2020-08-14 13:03:54,859 Epoch 144 finished. Val loss 0.06309549510478973, Val error 0.0199\n2020-08-14 13:04:02,582 Epoch 145 finished. Val loss 0.06202146038413048, Val error 0.0194\n2020-08-14 13:04:11,479 Epoch 146 finished. Val loss 0.06458515673875809, Val error 0.0213\n2020-08-14 13:04:20,050 Epoch 147 finished. Val loss 0.06916205585002899, Val error 0.0227\n2020-08-14 13:04:27,884 Epoch 148 finished. Val loss 0.06595040112733841, Val error 0.0201\n2020-08-14 13:04:35,798 Epoch 149 finished. Val loss 0.06734904646873474, Val error 0.022\n2020-08-14 13:04:43,763 Epoch 150 finished. Val loss 0.060271892696619034, Val error 0.0186\n2020-08-14 13:04:51,583 Epoch 151 finished. Val loss 0.06526493281126022, Val error 0.0201\n2020-08-14 13:04:59,391 Epoch 152 finished. Val loss 0.06542744487524033, Val error 0.0224\n2020-08-14 13:05:07,183 Epoch 153 finished. Val loss 0.06662287563085556, Val error 0.0201\n2020-08-14 13:05:14,944 Epoch 154 finished. Val loss 0.0710785984992981, Val error 0.0226\n2020-08-14 13:05:22,730 Epoch 155 finished. Val loss 0.06502063572406769, Val error 0.0205\n2020-08-14 13:05:30,601 Epoch 156 finished. Val loss 0.07099469006061554, Val error 0.0238\n2020-08-14 13:05:38,450 Epoch 157 finished. Val loss 0.07496901601552963, Val error 0.0233\n2020-08-14 13:05:46,369 Epoch 158 finished. Val loss 0.07504939287900925, Val error 0.0218\n2020-08-14 13:05:54,169 Epoch 159 finished. Val loss 0.06551120430231094, Val error 0.0198\n2020-08-14 13:06:02,260 Epoch 160 finished. Val loss 0.0663127526640892, Val error 0.0207\n2020-08-14 13:06:10,015 Epoch 161 finished. Val loss 0.07531759887933731, Val error 0.0246\n2020-08-14 13:06:17,811 Epoch 162 finished. Val loss 0.06557411700487137, Val error 0.0208\n2020-08-14 13:06:25,748 Epoch 163 finished. Val loss 0.06540769338607788, Val error 0.0201\n2020-08-14 13:06:33,758 Epoch 164 finished. Val loss 0.06289983540773392, Val error 0.02\n2020-08-14 13:06:41,516 Epoch 165 finished. Val loss 0.06414934992790222, Val error 0.0214\n2020-08-14 13:06:49,286 Epoch 166 finished. Val loss 0.06779002398252487, Val error 0.0211\n2020-08-14 13:06:57,207 Epoch 167 finished. Val loss 0.06446043401956558, Val error 0.0198\n2020-08-14 13:07:04,940 Epoch 168 finished. Val loss 0.06241266429424286, Val error 0.0192\n2020-08-14 13:07:12,724 Epoch 169 finished. Val loss 0.06268217414617538, Val error 0.021\n2020-08-14 13:07:20,498 Epoch 170 finished. Val loss 0.06704073399305344, Val error 0.0201\n2020-08-14 13:07:28,317 Epoch 171 finished. Val loss 0.07592198252677917, Val error 0.0232\n2020-08-14 13:07:36,091 Epoch 172 finished. Val loss 0.06136306747794151, Val error 0.0197\n2020-08-14 13:07:44,012 Epoch 173 finished. Val loss 0.06429712474346161, Val error 0.0204\n2020-08-14 13:07:51,854 Epoch 174 finished. Val loss 0.07026326656341553, Val error 0.0207\n2020-08-14 13:08:02,801 Epoch 175 finished. Val loss 0.06615189462900162, Val error 0.0194\n2020-08-14 13:08:12,903 Epoch 176 finished. Val loss 0.06719408929347992, Val error 0.0217\n2020-08-14 13:08:23,121 Epoch 177 finished. Val loss 0.0641198456287384, Val error 0.0188\n2020-08-14 13:08:33,111 Epoch 178 finished. Val loss 0.05874815583229065, Val error 0.0184\n2020-08-14 13:08:42,317 Epoch 179 finished. Val loss 0.06535211205482483, Val error 0.0192\n2020-08-14 13:08:50,872 Epoch 180 finished. Val loss 0.07792161405086517, Val error 0.024\n2020-08-14 13:08:58,588 Epoch 181 finished. Val loss 0.06858813762664795, Val error 0.0228\n2020-08-14 13:09:06,230 Epoch 182 finished. Val loss 0.062204509973526, Val error 0.0201\n2020-08-14 13:09:15,467 Epoch 183 finished. Val loss 0.059097807854413986, Val error 0.0191\n2020-08-14 13:09:29,393 Epoch 184 finished. Val loss 0.0718262791633606, Val error 0.0214\n2020-08-14 13:09:37,319 Epoch 185 finished. Val loss 0.05916208028793335, Val error 0.0187\n2020-08-14 13:09:45,153 Epoch 186 finished. Val loss 0.059893134981393814, Val error 0.0192\n2020-08-14 13:09:52,761 Epoch 187 finished. Val loss 0.06431125849485397, Val error 0.0192\n2020-08-14 13:10:00,446 Epoch 188 finished. Val loss 0.0656416118144989, Val error 0.0203\n2020-08-14 13:10:08,332 Epoch 189 finished. Val loss 0.0661102682352066, Val error 0.0201\n2020-08-14 13:10:17,210 Epoch 190 finished. Val loss 0.06665005534887314, Val error 0.0199\n2020-08-14 13:10:26,850 Epoch 191 finished. Val loss 0.06599871069192886, Val error 0.0201\n2020-08-14 13:10:34,699 Epoch 192 finished. Val loss 0.061371684074401855, Val error 0.0194\n2020-08-14 13:10:42,667 Epoch 193 finished. Val loss 0.06871997565031052, Val error 0.022\n2020-08-14 13:10:50,566 Epoch 194 finished. Val loss 0.06797640025615692, Val error 0.0192\n2020-08-14 13:10:58,849 Epoch 195 finished. Val loss 0.06473758071660995, Val error 0.0208\n2020-08-14 13:11:11,592 Epoch 196 finished. Val loss 0.06426460295915604, Val error 0.0187\n2020-08-14 13:11:23,228 Epoch 197 finished. Val loss 0.06279231607913971, Val error 0.0204\n2020-08-14 13:11:33,444 Epoch 198 finished. Val loss 0.06841659545898438, Val error 0.0223\n2020-08-14 13:11:43,363 Epoch 199 finished. Val loss 0.07154714316129684, Val error 0.0208\n2020-08-14 13:11:54,503 Epoch 200 finished. Val loss 0.06662601977586746, Val error 0.021\n2020-08-14 13:12:04,463 Epoch 201 finished. Val loss 0.06682173907756805, Val error 0.02\n2020-08-14 13:12:12,396 Epoch 202 finished. Val loss 0.06283071637153625, Val error 0.019\n2020-08-14 13:12:20,229 Epoch 203 finished. Val loss 0.06596479564905167, Val error 0.0193\n2020-08-14 13:12:28,043 Epoch 204 finished. Val loss 0.06851057708263397, Val error 0.0221\n2020-08-14 13:12:35,822 Epoch 205 finished. Val loss 0.059288352727890015, Val error 0.0173\n2020-08-14 13:12:43,573 Epoch 206 finished. Val loss 0.06514979898929596, Val error 0.0207\n2020-08-14 13:12:51,355 Epoch 207 finished. Val loss 0.060890503227710724, Val error 0.0191\n2020-08-14 13:12:59,235 Epoch 208 finished. Val loss 0.07349250465631485, Val error 0.0233\n2020-08-14 13:13:07,010 Epoch 209 finished. Val loss 0.06643806397914886, Val error 0.0195\n2020-08-14 13:13:15,027 Epoch 210 finished. Val loss 0.07165108621120453, Val error 0.0215\n2020-08-14 13:13:22,731 Epoch 211 finished. Val loss 0.08013225346803665, Val error 0.0249\n2020-08-14 13:13:30,674 Epoch 212 finished. Val loss 0.06958100944757462, Val error 0.0208\n2020-08-14 13:13:39,311 Epoch 213 finished. Val loss 0.07206609100103378, Val error 0.0225\n2020-08-14 13:13:47,683 Epoch 214 finished. Val loss 0.06553836911916733, Val error 0.0206\n2020-08-14 13:13:57,181 Epoch 215 finished. Val loss 0.06356652081012726, Val error 0.0199\n2020-08-14 13:14:05,701 Epoch 216 finished. Val loss 0.0629863366484642, Val error 0.0194\n2020-08-14 13:14:13,552 Epoch 217 finished. Val loss 0.07067367434501648, Val error 0.0211\n2020-08-14 13:14:21,693 Epoch 218 finished. Val loss 0.06996625661849976, Val error 0.0207\n2020-08-14 13:14:29,828 Epoch 219 finished. Val loss 0.05728819593787193, Val error 0.0172\n2020-08-14 13:14:39,145 Epoch 220 finished. Val loss 0.061182670295238495, Val error 0.0196\n2020-08-14 13:14:47,097 Epoch 221 finished. Val loss 0.07185673713684082, Val error 0.0226\n2020-08-14 13:14:55,785 Epoch 222 finished. Val loss 0.06670187413692474, Val error 0.0198\n2020-08-14 13:15:05,044 Epoch 223 finished. Val loss 0.06468434631824493, Val error 0.0193\n2020-08-14 13:15:13,950 Epoch 224 finished. Val loss 0.06168357655405998, Val error 0.018\n2020-08-14 13:15:23,092 Epoch 225 finished. Val loss 0.07140010595321655, Val error 0.0231\n2020-08-14 13:15:32,899 Epoch 226 finished. Val loss 0.07156722247600555, Val error 0.0215\n2020-08-14 13:15:42,578 Epoch 227 finished. Val loss 0.06573666632175446, Val error 0.0201\n2020-08-14 13:15:52,739 Epoch 228 finished. Val loss 0.07025764882564545, Val error 0.0216\n2020-08-14 13:16:01,663 Epoch 229 finished. Val loss 0.06743664294481277, Val error 0.0217\n2020-08-14 13:16:09,349 Epoch 230 finished. Val loss 0.06046051159501076, Val error 0.0187\n2020-08-14 13:16:17,114 Epoch 231 finished. Val loss 0.06301405280828476, Val error 0.0199\n2020-08-14 13:16:28,655 Epoch 232 finished. Val loss 0.06749572604894638, Val error 0.0196\n2020-08-14 13:16:38,205 Epoch 233 finished. Val loss 0.06304788589477539, Val error 0.0204\n2020-08-14 13:16:46,724 Epoch 234 finished. Val loss 0.06775354593992233, Val error 0.0205\n2020-08-14 13:16:56,680 Epoch 235 finished. Val loss 0.0660567581653595, Val error 0.0205\n2020-08-14 13:17:07,188 Epoch 236 finished. Val loss 0.06926823407411575, Val error 0.0214\n2020-08-14 13:17:15,044 Epoch 237 finished. Val loss 0.067223459482193, Val error 0.0213\n2020-08-14 13:17:23,033 Epoch 238 finished. Val loss 0.06812191754579544, Val error 0.0201\n2020-08-14 13:17:30,934 Epoch 239 finished. Val loss 0.06444453448057175, Val error 0.0201\n2020-08-14 13:17:38,704 Epoch 240 finished. Val loss 0.06035034731030464, Val error 0.0179\n2020-08-14 13:17:46,949 Epoch 241 finished. Val loss 0.06669111549854279, Val error 0.0199\n2020-08-14 13:17:56,449 Epoch 242 finished. Val loss 0.06308160722255707, Val error 0.0199\n2020-08-14 13:18:04,386 Epoch 243 finished. Val loss 0.057617396116256714, Val error 0.0188\n2020-08-14 13:18:12,286 Epoch 244 finished. Val loss 0.07476384937763214, Val error 0.0238\n2020-08-14 13:18:20,071 Epoch 245 finished. Val loss 0.06709153950214386, Val error 0.0218\n2020-08-14 13:18:28,043 Epoch 246 finished. Val loss 0.06571953743696213, Val error 0.0204\n2020-08-14 13:18:35,932 Epoch 247 finished. Val loss 0.06564104557037354, Val error 0.0194\n2020-08-14 13:18:43,831 Epoch 248 finished. Val loss 0.06531064212322235, Val error 0.0191\n2020-08-14 13:18:51,665 Epoch 249 finished. Val loss 0.06611635535955429, Val error 0.0188\n2020-08-14 13:18:59,583 Epoch 250 finished. Val loss 0.05974495783448219, Val error 0.0176\n2020-08-14 13:19:07,415 Epoch 251 finished. Val loss 0.06580933183431625, Val error 0.0206\n2020-08-14 13:19:15,322 Epoch 252 finished. Val loss 0.06796321272850037, Val error 0.0193\n2020-08-14 13:19:23,312 Epoch 253 finished. Val loss 0.07572299242019653, Val error 0.0239\n2020-08-14 13:19:31,665 Epoch 254 finished. Val loss 0.06418604403734207, Val error 0.0198\n2020-08-14 13:19:39,614 Epoch 255 finished. Val loss 0.06244007498025894, Val error 0.0187\n2020-08-14 13:19:47,503 Epoch 256 finished. Val loss 0.061966270208358765, Val error 0.0201\n2020-08-14 13:19:55,652 Epoch 257 finished. Val loss 0.05910343676805496, Val error 0.0174\n2020-08-14 13:20:03,691 Epoch 258 finished. Val loss 0.06378983706235886, Val error 0.0196\n2020-08-14 13:20:11,608 Epoch 259 finished. Val loss 0.06052831560373306, Val error 0.0193\n2020-08-14 13:20:19,650 Epoch 260 finished. Val loss 0.06322608888149261, Val error 0.0187\n2020-08-14 13:20:27,657 Epoch 261 finished. Val loss 0.056806132197380066, Val error 0.0179\n2020-08-14 13:20:35,690 Epoch 262 finished. Val loss 0.06223723292350769, Val error 0.0197\n2020-08-14 13:20:43,536 Epoch 263 finished. Val loss 0.07219734787940979, Val error 0.0234\n2020-08-14 13:20:51,458 Epoch 264 finished. Val loss 0.06323479115962982, Val error 0.019\n2020-08-14 13:20:59,454 Epoch 265 finished. Val loss 0.06301906704902649, Val error 0.0191\n2020-08-14 13:21:07,376 Epoch 266 finished. Val loss 0.06548751145601273, Val error 0.0195\n2020-08-14 13:21:15,201 Epoch 267 finished. Val loss 0.07431188970804214, Val error 0.0224\n2020-08-14 13:21:24,354 Epoch 268 finished. Val loss 0.06531352549791336, Val error 0.0203\n2020-08-14 13:21:37,098 Epoch 269 finished. Val loss 0.06267481297254562, Val error 0.019\n2020-08-14 13:21:47,025 Epoch 270 finished. Val loss 0.06389833241701126, Val error 0.0198\n2020-08-14 13:21:55,100 Epoch 271 finished. Val loss 0.06616784632205963, Val error 0.021\n2020-08-14 13:22:04,267 Epoch 272 finished. Val loss 0.06633590161800385, Val error 0.0193\n2020-08-14 13:22:16,559 Epoch 273 finished. Val loss 0.06617173552513123, Val error 0.0214\n2020-08-14 13:22:27,203 Epoch 274 finished. Val loss 0.06516409665346146, Val error 0.0195\n2020-08-14 13:22:37,886 Epoch 275 finished. Val loss 0.07652105391025543, Val error 0.0229\n2020-08-14 13:22:48,631 Epoch 276 finished. Val loss 0.06575474143028259, Val error 0.0206\n2020-08-14 13:23:00,992 Epoch 277 finished. Val loss 0.06444866210222244, Val error 0.0188\n2020-08-14 13:23:10,759 Epoch 278 finished. Val loss 0.06895843148231506, Val error 0.0219\n2020-08-14 13:23:23,879 Epoch 279 finished. Val loss 0.06266100704669952, Val error 0.0189\n2020-08-14 13:23:32,653 Epoch 280 finished. Val loss 0.07044032961130142, Val error 0.0231\n2020-08-14 13:23:44,695 Epoch 281 finished. Val loss 0.06679517775774002, Val error 0.0199\n2020-08-14 13:23:55,540 Epoch 282 finished. Val loss 0.0592619925737381, Val error 0.0179\n2020-08-14 13:24:06,912 Epoch 283 finished. Val loss 0.06979358196258545, Val error 0.0216\n2020-08-14 13:24:18,814 Epoch 284 finished. Val loss 0.06488043814897537, Val error 0.0196\n2020-08-14 13:24:31,642 Epoch 285 finished. Val loss 0.07208699733018875, Val error 0.0209\n2020-08-14 13:24:46,519 Epoch 286 finished. Val loss 0.06653807312250137, Val error 0.0194\n2020-08-14 13:25:00,082 Epoch 287 finished. Val loss 0.06629950553178787, Val error 0.0204\n2020-08-14 13:25:08,153 Epoch 288 finished. Val loss 0.06557826697826385, Val error 0.0201\n2020-08-14 13:25:19,101 Epoch 289 finished. Val loss 0.06983668357133865, Val error 0.0231\n2020-08-14 13:25:28,913 Epoch 290 finished. Val loss 0.06448621302843094, Val error 0.0186\n2020-08-14 13:25:40,962 Epoch 291 finished. Val loss 0.0665682926774025, Val error 0.0189\n2020-08-14 13:25:50,970 Epoch 292 finished. Val loss 0.07014413177967072, Val error 0.0208\n2020-08-14 13:26:03,786 Epoch 293 finished. Val loss 0.0674782246351242, Val error 0.0211\n2020-08-14 13:26:14,703 Epoch 294 finished. Val loss 0.06434899568557739, Val error 0.0192\n2020-08-14 13:26:22,794 Epoch 295 finished. Val loss 0.06596119701862335, Val error 0.0202\n2020-08-14 13:26:32,503 Epoch 296 finished. Val loss 0.06573452055454254, Val error 0.0198\n2020-08-14 13:26:42,630 Epoch 297 finished. Val loss 0.06097352132201195, Val error 0.0191\n2020-08-14 13:26:54,633 Epoch 298 finished. Val loss 0.0647987574338913, Val error 0.0192\n2020-08-14 13:27:03,809 Epoch 299 finished. Val loss 0.0604061633348465, Val error 0.0186\n"
    }
   ],
   "source": [
    "trainer.train(n_epoch=n_epoch, burn_in_epochs=burn_in_epochs, resample_prior_until=resample_prior_until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100\n"
    }
   ],
   "source": [
    "weights_set = trainer.weight_set_samples[-(n_epoch - resample_prior_until) // save_freq:]\n",
    "\n",
    "print(len(weights_set))\n",
    "\n",
    "Path('../saved_samples', 'mnist_weights').mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(weights_set, Path('../saved_samples', 'mnist_weights', 'weights.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_set = weights_set[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10"
     },
     "metadata": {},
     "execution_count": 354
    }
   ],
   "source": [
    "len(weights_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict_to_vec(state_dict):\n",
    "    return torch.cat([w_i.view(-1) for w_i in state_dict.values()])\n",
    "\n",
    "squeezed_weights = [state_dict_to_vec(w) for w in weights_set]\n",
    "\n",
    "models = [MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim) for w in weights_set]\n",
    "for w, model in zip(weights_set, models):\n",
    "    model.load_state_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_with_priors = trainer.optimizer\n",
    "\n",
    "priors = {}\n",
    "group_params = opt_with_priors.param_groups[0]['params']\n",
    "for (n, _), p in zip(model.named_parameters(), group_params):  \n",
    "    state = opt_with_priors.state[p]  \n",
    "    priors[n] = state['weight_decay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x, model):\n",
    "    return F.softmax(model(x), dim=-1)\n",
    "\n",
    "def get_binary_prediction(x, model, classes):\n",
    "    assert len(classes) == 2\n",
    "    return F.softmax(model(x)[..., classes], dim=-1)[..., -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(x, y, model):\n",
    "    y_hat = model(x)\n",
    "    log_likelihood = -F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    return log_likelihood\n",
    "\n",
    "def compute_mc_estimate(function: callable, models, x: torch.tensor):\n",
    "    res = 0.0\n",
    "    for model in models:\n",
    "        res += function(x, model)\n",
    "    return res / len(models)\n",
    "\n",
    "def compute_naive_variance(function:callable, control_variate: callable, models, x: torch.tensor):\n",
    "    sample_mean = compute_mc_estimate(lambda x_, model: function(x_, model) - control_variate(x_, model), models, x)\n",
    "    v = 0\n",
    "    for model in models:\n",
    "        v += (function(x, model) - control_variate(x, model) - sample_mean)**2 / (len(models) - 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tricky_divergence(model):\n",
    "    div = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            div += p.grad.sum()\n",
    "\n",
    "    return div\n",
    "\n",
    "def stein_control_variate(psy_model, model, train_x, train_y, new_x, priors, N_train):\n",
    "    model.zero_grad()\n",
    "    log_likelihood = compute_log_likelihood(train_x, train_y, model) * N_train\n",
    "    log_likelihood.backward()\n",
    "    \n",
    "    ncv_value = 0\n",
    "    psy_value = psy_model(state_dict_to_vec(model.state_dict()), x_new)\n",
    "    psy_value.backward(retain_graph=True)\n",
    "    psy_div = compute_tricky_divergence(psy_model)\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            d_p = p.grad.data\n",
    "            d_p.add_(p.data, alpha=-priors[n])\n",
    "\n",
    "            ncv_value += d_p.sum()\n",
    "\n",
    "    ncv_value = torch.cat([ncv_value.view(1)]*psy_value.shape[0], dim=0)\n",
    "\n",
    "    ncv_value *= psy_value\n",
    "    ncv_value += psy_div\n",
    "\n",
    "    return ncv_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PsyLinear(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        return  self.layer(weights)\n",
    "\n",
    "class PsyMLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.ReLU()]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(width, 1, bias=False))\n",
    "        #layers.append(nn.Tanh())\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "        #for p in self.parameters():\n",
    "        #    torch.nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        return self.block(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_hidden = 30\n",
    "n_iter = 100\n",
    "psy_lr = 1e-4\n",
    "psy_input = squeezed_weights[0].shape[0]\n",
    "N_train = batch_size * len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = next(iter(valloader))\n",
    "x_new = x_new[:1]\n",
    "train_x, train_y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_model = PsyMLP(psy_input, psy_hidden, 1)\n",
    "psy_model.to(device)\n",
    "\n",
    "neural_control_variate = lambda x, model : stein_control_variate(psy_model, model, train_x, train_y, x, priors, N_train)\n",
    "\n",
    "ncv_optimizer = torch.optim.Adam(psy_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.0068], grad_fn=<SqueezeBackward3>)"
     },
     "metadata": {},
     "execution_count": 492
    }
   ],
   "source": [
    "psy_model(squeezed_weights[0], x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([11993.8486], grad_fn=<AddBackward0>)\ntensor([2.7918e+09], grad_fn=<AddBackward0>)\ntensor([1.7531e+08], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\ntensor([0.0232], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "function_f = lambda x, model: get_binary_prediction(x, model, classes=[3, 5])\n",
    "\n",
    "for it in range(n_iter):\n",
    "    ncv_optimizer.zero_grad()\n",
    "    mc_variance = compute_naive_variance(function_f, neural_control_variate, models, x_new)\n",
    "    print(mc_variance)\n",
    "    mc_variance.backward()\n",
    "    ncv_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n, p in psy_model.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}