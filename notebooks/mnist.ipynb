{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from control_variates.model import MLP\n",
    "from control_variates.optim import LangevinSGD as SGLD, ScaleAdaSGHMC as H_SA_SGHMC\n",
    "from mnist_utils import load_mnist_dataset\n",
    "from control_variates.trainer import BNNTrainer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from pathlib import Path\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "input_dim = 784\n",
    "width = 100\n",
    "depth = 2\n",
    "output_dim = 10\n",
    "lr = 1e-3\n",
    "n_epoch = 300\n",
    "alpha0, beta0 = 10, 10\n",
    "resample_prior_every = 15\n",
    "resample_momentum_every = 50\n",
    "burn_in_epochs = 20\n",
    "save_freq = 2\n",
    "resample_prior_until = 100\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Path('../data', 'mnist').mkdir(exist_ok=True, parents=True)\n",
    "trainloader, valloader = load_mnist_dataset(Path('../data', 'mnist'), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGLD(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "optimizer = H_SA_SGHMC(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_func(y_hat, y):\n",
    "    nll = F.cross_entropy(y_hat, y, reduction='sum')\n",
    "    return nll\n",
    "\n",
    "def err_func(y_hat, y):\n",
    "    err = y_hat.argmax(-1).ne(y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BNNTrainer(model, optimizer, nll_func, err_func, trainloader, valloader, device=device, \n",
    "    resample_prior_every=resample_prior_every,\n",
    "    resample_momentum_every=resample_momentum_every,\n",
    "    save_freq=save_freq,\n",
    "    batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": " loss 0.06701449304819107, Val error 0.0199\n2020-08-14 12:53:41,426 Epoch 80 finished. Val loss 0.07195600867271423, Val error 0.0225\n2020-08-14 12:53:49,692 Epoch 81 finished. Val loss 0.07047393172979355, Val error 0.0203\n2020-08-14 12:53:58,345 Epoch 82 finished. Val loss 0.0689258798956871, Val error 0.0214\n2020-08-14 12:54:07,573 Epoch 83 finished. Val loss 0.07056793570518494, Val error 0.0219\n2020-08-14 12:54:16,372 Epoch 84 finished. Val loss 0.06595133990049362, Val error 0.0207\n2020-08-14 12:54:27,838 Epoch 85 finished. Val loss 0.0649980679154396, Val error 0.0202\n2020-08-14 12:54:37,796 Epoch 86 finished. Val loss 0.07060126960277557, Val error 0.0218\n2020-08-14 12:54:49,623 Epoch 87 finished. Val loss 0.0725543275475502, Val error 0.0226\n2020-08-14 12:54:58,163 Epoch 88 finished. Val loss 0.05860758200287819, Val error 0.0185\n2020-08-14 12:55:06,278 Epoch 89 finished. Val loss 0.06630147993564606, Val error 0.0204\n2020-08-14 12:55:14,271 Epoch 90 finished. Val loss 0.06601616740226746, Val error 0.0196\n2020-08-14 12:55:22,381 Epoch 91 finished. Val loss 0.07027024030685425, Val error 0.0216\n2020-08-14 12:55:30,307 Epoch 92 finished. Val loss 0.06825648248195648, Val error 0.0217\n2020-08-14 12:55:39,623 Epoch 93 finished. Val loss 0.06944569945335388, Val error 0.0225\n2020-08-14 12:55:49,736 Epoch 94 finished. Val loss 0.06596875190734863, Val error 0.0198\n2020-08-14 12:55:58,875 Epoch 95 finished. Val loss 0.06656596064567566, Val error 0.02\n2020-08-14 12:56:08,337 Epoch 96 finished. Val loss 0.07258154451847076, Val error 0.0226\n2020-08-14 12:56:19,103 Epoch 97 finished. Val loss 0.07665721327066422, Val error 0.024\n2020-08-14 12:56:28,756 Epoch 98 finished. Val loss 0.06241890415549278, Val error 0.0193\n2020-08-14 12:56:37,691 Epoch 99 finished. Val loss 0.06611023843288422, Val error 0.0205\n2020-08-14 12:56:47,009 Epoch 100 finished. Val loss 0.0686938688158989, Val error 0.0212\n2020-08-14 12:56:54,936 Epoch 101 finished. Val loss 0.06812462955713272, Val error 0.0216\n2020-08-14 12:57:03,107 Epoch 102 finished. Val loss 0.06681443005800247, Val error 0.0208\n2020-08-14 12:57:13,876 Epoch 103 finished. Val loss 0.06180604174733162, Val error 0.0181\n2020-08-14 12:57:23,484 Epoch 104 finished. Val loss 0.06892265379428864, Val error 0.0207\n2020-08-14 12:57:31,507 Epoch 105 finished. Val loss 0.0594828724861145, Val error 0.0168\n2020-08-14 12:57:40,300 Epoch 106 finished. Val loss 0.07314226031303406, Val error 0.0227\n2020-08-14 12:57:48,785 Epoch 107 finished. Val loss 0.06853685528039932, Val error 0.0203\n2020-08-14 12:57:59,291 Epoch 108 finished. Val loss 0.06422159075737, Val error 0.0184\n2020-08-14 12:58:08,783 Epoch 109 finished. Val loss 0.0620708242058754, Val error 0.0192\n2020-08-14 12:58:19,525 Epoch 110 finished. Val loss 0.0631440207362175, Val error 0.0199\n2020-08-14 12:58:29,778 Epoch 111 finished. Val loss 0.06106937304139137, Val error 0.0179\n2020-08-14 12:58:39,439 Epoch 112 finished. Val loss 0.07099727541208267, Val error 0.0209\n2020-08-14 12:58:48,437 Epoch 113 finished. Val loss 0.06403850764036179, Val error 0.0212\n2020-08-14 12:58:57,152 Epoch 114 finished. Val loss 0.07271837443113327, Val error 0.0215\n2020-08-14 12:59:07,650 Epoch 115 finished. Val loss 0.06390925496816635, Val error 0.0194\n2020-08-14 12:59:18,186 Epoch 116 finished. Val loss 0.0690336599946022, Val error 0.0221\n2020-08-14 12:59:30,284 Epoch 117 finished. Val loss 0.06347627192735672, Val error 0.0209\n2020-08-14 12:59:41,845 Epoch 118 finished. Val loss 0.06614278256893158, Val error 0.0208\n2020-08-14 12:59:52,343 Epoch 119 finished. Val loss 0.06783705949783325, Val error 0.0208\n2020-08-14 13:00:01,947 Epoch 120 finished. Val loss 0.06772395223379135, Val error 0.0206\n2020-08-14 13:00:11,670 Epoch 121 finished. Val loss 0.06586994230747223, Val error 0.0216\n2020-08-14 13:00:20,547 Epoch 122 finished. Val loss 0.06233235448598862, Val error 0.0183\n2020-08-14 13:00:29,458 Epoch 123 finished. Val loss 0.06694565713405609, Val error 0.02\n2020-08-14 13:00:38,152 Epoch 124 finished. Val loss 0.07029296457767487, Val error 0.0216\n2020-08-14 13:00:46,812 Epoch 125 finished. Val loss 0.0649881362915039, Val error 0.0217\n2020-08-14 13:00:58,140 Epoch 126 finished. Val loss 0.07536768913269043, Val error 0.0234\n2020-08-14 13:01:09,647 Epoch 127 finished. Val loss 0.06851137429475784, Val error 0.0217\n2020-08-14 13:01:18,286 Epoch 128 finished. Val loss 0.06368386745452881, Val error 0.0195\n2020-08-14 13:01:29,172 Epoch 129 finished. Val loss 0.0638662576675415, Val error 0.0204\n2020-08-14 13:01:38,829 Epoch 130 finished. Val loss 0.06909731775522232, Val error 0.0219\n2020-08-14 13:01:49,797 Epoch 131 finished. Val loss 0.06488369405269623, Val error 0.0196\n2020-08-14 13:02:02,263 Epoch 132 finished. Val loss 0.0638352632522583, Val error 0.0203\n2020-08-14 13:02:13,381 Epoch 133 finished. Val loss 0.06433234363794327, Val error 0.0187\n2020-08-14 13:02:22,953 Epoch 134 finished. Val loss 0.06675011664628983, Val error 0.02\n2020-08-14 13:02:30,958 Epoch 135 finished. Val loss 0.06441117823123932, Val error 0.0189\n2020-08-14 13:02:39,875 Epoch 136 finished. Val loss 0.0681566521525383, Val error 0.0217\n2020-08-14 13:02:49,352 Epoch 137 finished. Val loss 0.06189943104982376, Val error 0.0195\n2020-08-14 13:02:57,670 Epoch 138 finished. Val loss 0.06684070080518723, Val error 0.0211\n2020-08-14 13:03:08,135 Epoch 139 finished. Val loss 0.06143581494688988, Val error 0.0174\n2020-08-14 13:03:20,591 Epoch 140 finished. Val loss 0.06716150045394897, Val error 0.0209\n2020-08-14 13:03:31,828 Epoch 141 finished. Val loss 0.07549871504306793, Val error 0.0229\n2020-08-14 13:03:39,485 Epoch 142 finished. Val loss 0.07043377310037613, Val error 0.0224\n2020-08-14 13:03:47,179 Epoch 143 finished. Val loss 0.0646219328045845, Val error 0.0201\n2020-08-14 13:03:54,859 Epoch 144 finished. Val loss 0.06309549510478973, Val error 0.0199\n2020-08-14 13:04:02,582 Epoch 145 finished. Val loss 0.06202146038413048, Val error 0.0194\n2020-08-14 13:04:11,479 Epoch 146 finished. Val loss 0.06458515673875809, Val error 0.0213\n2020-08-14 13:04:20,050 Epoch 147 finished. Val loss 0.06916205585002899, Val error 0.0227\n2020-08-14 13:04:27,884 Epoch 148 finished. Val loss 0.06595040112733841, Val error 0.0201\n2020-08-14 13:04:35,798 Epoch 149 finished. Val loss 0.06734904646873474, Val error 0.022\n2020-08-14 13:04:43,763 Epoch 150 finished. Val loss 0.060271892696619034, Val error 0.0186\n2020-08-14 13:04:51,583 Epoch 151 finished. Val loss 0.06526493281126022, Val error 0.0201\n2020-08-14 13:04:59,391 Epoch 152 finished. Val loss 0.06542744487524033, Val error 0.0224\n2020-08-14 13:05:07,183 Epoch 153 finished. Val loss 0.06662287563085556, Val error 0.0201\n2020-08-14 13:05:14,944 Epoch 154 finished. Val loss 0.0710785984992981, Val error 0.0226\n2020-08-14 13:05:22,730 Epoch 155 finished. Val loss 0.06502063572406769, Val error 0.0205\n2020-08-14 13:05:30,601 Epoch 156 finished. Val loss 0.07099469006061554, Val error 0.0238\n2020-08-14 13:05:38,450 Epoch 157 finished. Val loss 0.07496901601552963, Val error 0.0233\n2020-08-14 13:05:46,369 Epoch 158 finished. Val loss 0.07504939287900925, Val error 0.0218\n2020-08-14 13:05:54,169 Epoch 159 finished. Val loss 0.06551120430231094, Val error 0.0198\n2020-08-14 13:06:02,260 Epoch 160 finished. Val loss 0.0663127526640892, Val error 0.0207\n2020-08-14 13:06:10,015 Epoch 161 finished. Val loss 0.07531759887933731, Val error 0.0246\n2020-08-14 13:06:17,811 Epoch 162 finished. Val loss 0.06557411700487137, Val error 0.0208\n2020-08-14 13:06:25,748 Epoch 163 finished. Val loss 0.06540769338607788, Val error 0.0201\n2020-08-14 13:06:33,758 Epoch 164 finished. Val loss 0.06289983540773392, Val error 0.02\n2020-08-14 13:06:41,516 Epoch 165 finished. Val loss 0.06414934992790222, Val error 0.0214\n2020-08-14 13:06:49,286 Epoch 166 finished. Val loss 0.06779002398252487, Val error 0.0211\n2020-08-14 13:06:57,207 Epoch 167 finished. Val loss 0.06446043401956558, Val error 0.0198\n2020-08-14 13:07:04,940 Epoch 168 finished. Val loss 0.06241266429424286, Val error 0.0192\n2020-08-14 13:07:12,724 Epoch 169 finished. Val loss 0.06268217414617538, Val error 0.021\n2020-08-14 13:07:20,498 Epoch 170 finished. Val loss 0.06704073399305344, Val error 0.0201\n2020-08-14 13:07:28,317 Epoch 171 finished. Val loss 0.07592198252677917, Val error 0.0232\n2020-08-14 13:07:36,091 Epoch 172 finished. Val loss 0.06136306747794151, Val error 0.0197\n2020-08-14 13:07:44,012 Epoch 173 finished. Val loss 0.06429712474346161, Val error 0.0204\n2020-08-14 13:07:51,854 Epoch 174 finished. Val loss 0.07026326656341553, Val error 0.0207\n2020-08-14 13:08:02,801 Epoch 175 finished. Val loss 0.06615189462900162, Val error 0.0194\n2020-08-14 13:08:12,903 Epoch 176 finished. Val loss 0.06719408929347992, Val error 0.0217\n2020-08-14 13:08:23,121 Epoch 177 finished. Val loss 0.0641198456287384, Val error 0.0188\n2020-08-14 13:08:33,111 Epoch 178 finished. Val loss 0.05874815583229065, Val error 0.0184\n2020-08-14 13:08:42,317 Epoch 179 finished. Val loss 0.06535211205482483, Val error 0.0192\n2020-08-14 13:08:50,872 Epoch 180 finished. Val loss 0.07792161405086517, Val error 0.024\n2020-08-14 13:08:58,588 Epoch 181 finished. Val loss 0.06858813762664795, Val error 0.0228\n2020-08-14 13:09:06,230 Epoch 182 finished. Val loss 0.062204509973526, Val error 0.0201\n2020-08-14 13:09:15,467 Epoch 183 finished. Val loss 0.059097807854413986, Val error 0.0191\n2020-08-14 13:09:29,393 Epoch 184 finished. Val loss 0.0718262791633606, Val error 0.0214\n2020-08-14 13:09:37,319 Epoch 185 finished. Val loss 0.05916208028793335, Val error 0.0187\n2020-08-14 13:09:45,153 Epoch 186 finished. Val loss 0.059893134981393814, Val error 0.0192\n2020-08-14 13:09:52,761 Epoch 187 finished. Val loss 0.06431125849485397, Val error 0.0192\n2020-08-14 13:10:00,446 Epoch 188 finished. Val loss 0.0656416118144989, Val error 0.0203\n2020-08-14 13:10:08,332 Epoch 189 finished. Val loss 0.0661102682352066, Val error 0.0201\n2020-08-14 13:10:17,210 Epoch 190 finished. Val loss 0.06665005534887314, Val error 0.0199\n2020-08-14 13:10:26,850 Epoch 191 finished. Val loss 0.06599871069192886, Val error 0.0201\n2020-08-14 13:10:34,699 Epoch 192 finished. Val loss 0.061371684074401855, Val error 0.0194\n2020-08-14 13:10:42,667 Epoch 193 finished. Val loss 0.06871997565031052, Val error 0.022\n2020-08-14 13:10:50,566 Epoch 194 finished. Val loss 0.06797640025615692, Val error 0.0192\n2020-08-14 13:10:58,849 Epoch 195 finished. Val loss 0.06473758071660995, Val error 0.0208\n2020-08-14 13:11:11,592 Epoch 196 finished. Val loss 0.06426460295915604, Val error 0.0187\n2020-08-14 13:11:23,228 Epoch 197 finished. Val loss 0.06279231607913971, Val error 0.0204\n2020-08-14 13:11:33,444 Epoch 198 finished. Val loss 0.06841659545898438, Val error 0.0223\n2020-08-14 13:11:43,363 Epoch 199 finished. Val loss 0.07154714316129684, Val error 0.0208\n2020-08-14 13:11:54,503 Epoch 200 finished. Val loss 0.06662601977586746, Val error 0.021\n2020-08-14 13:12:04,463 Epoch 201 finished. Val loss 0.06682173907756805, Val error 0.02\n2020-08-14 13:12:12,396 Epoch 202 finished. Val loss 0.06283071637153625, Val error 0.019\n2020-08-14 13:12:20,229 Epoch 203 finished. Val loss 0.06596479564905167, Val error 0.0193\n2020-08-14 13:12:28,043 Epoch 204 finished. Val loss 0.06851057708263397, Val error 0.0221\n2020-08-14 13:12:35,822 Epoch 205 finished. Val loss 0.059288352727890015, Val error 0.0173\n2020-08-14 13:12:43,573 Epoch 206 finished. Val loss 0.06514979898929596, Val error 0.0207\n2020-08-14 13:12:51,355 Epoch 207 finished. Val loss 0.060890503227710724, Val error 0.0191\n2020-08-14 13:12:59,235 Epoch 208 finished. Val loss 0.07349250465631485, Val error 0.0233\n2020-08-14 13:13:07,010 Epoch 209 finished. Val loss 0.06643806397914886, Val error 0.0195\n2020-08-14 13:13:15,027 Epoch 210 finished. Val loss 0.07165108621120453, Val error 0.0215\n2020-08-14 13:13:22,731 Epoch 211 finished. Val loss 0.08013225346803665, Val error 0.0249\n2020-08-14 13:13:30,674 Epoch 212 finished. Val loss 0.06958100944757462, Val error 0.0208\n2020-08-14 13:13:39,311 Epoch 213 finished. Val loss 0.07206609100103378, Val error 0.0225\n2020-08-14 13:13:47,683 Epoch 214 finished. Val loss 0.06553836911916733, Val error 0.0206\n2020-08-14 13:13:57,181 Epoch 215 finished. Val loss 0.06356652081012726, Val error 0.0199\n2020-08-14 13:14:05,701 Epoch 216 finished. Val loss 0.0629863366484642, Val error 0.0194\n2020-08-14 13:14:13,552 Epoch 217 finished. Val loss 0.07067367434501648, Val error 0.0211\n2020-08-14 13:14:21,693 Epoch 218 finished. Val loss 0.06996625661849976, Val error 0.0207\n2020-08-14 13:14:29,828 Epoch 219 finished. Val loss 0.05728819593787193, Val error 0.0172\n2020-08-14 13:14:39,145 Epoch 220 finished. Val loss 0.061182670295238495, Val error 0.0196\n2020-08-14 13:14:47,097 Epoch 221 finished. Val loss 0.07185673713684082, Val error 0.0226\n2020-08-14 13:14:55,785 Epoch 222 finished. Val loss 0.06670187413692474, Val error 0.0198\n2020-08-14 13:15:05,044 Epoch 223 finished. Val loss 0.06468434631824493, Val error 0.0193\n2020-08-14 13:15:13,950 Epoch 224 finished. Val loss 0.06168357655405998, Val error 0.018\n2020-08-14 13:15:23,092 Epoch 225 finished. Val loss 0.07140010595321655, Val error 0.0231\n2020-08-14 13:15:32,899 Epoch 226 finished. Val loss 0.07156722247600555, Val error 0.0215\n2020-08-14 13:15:42,578 Epoch 227 finished. Val loss 0.06573666632175446, Val error 0.0201\n2020-08-14 13:15:52,739 Epoch 228 finished. Val loss 0.07025764882564545, Val error 0.0216\n2020-08-14 13:16:01,663 Epoch 229 finished. Val loss 0.06743664294481277, Val error 0.0217\n2020-08-14 13:16:09,349 Epoch 230 finished. Val loss 0.06046051159501076, Val error 0.0187\n2020-08-14 13:16:17,114 Epoch 231 finished. Val loss 0.06301405280828476, Val error 0.0199\n2020-08-14 13:16:28,655 Epoch 232 finished. Val loss 0.06749572604894638, Val error 0.0196\n2020-08-14 13:16:38,205 Epoch 233 finished. Val loss 0.06304788589477539, Val error 0.0204\n2020-08-14 13:16:46,724 Epoch 234 finished. Val loss 0.06775354593992233, Val error 0.0205\n2020-08-14 13:16:56,680 Epoch 235 finished. Val loss 0.0660567581653595, Val error 0.0205\n2020-08-14 13:17:07,188 Epoch 236 finished. Val loss 0.06926823407411575, Val error 0.0214\n2020-08-14 13:17:15,044 Epoch 237 finished. Val loss 0.067223459482193, Val error 0.0213\n2020-08-14 13:17:23,033 Epoch 238 finished. Val loss 0.06812191754579544, Val error 0.0201\n2020-08-14 13:17:30,934 Epoch 239 finished. Val loss 0.06444453448057175, Val error 0.0201\n2020-08-14 13:17:38,704 Epoch 240 finished. Val loss 0.06035034731030464, Val error 0.0179\n2020-08-14 13:17:46,949 Epoch 241 finished. Val loss 0.06669111549854279, Val error 0.0199\n2020-08-14 13:17:56,449 Epoch 242 finished. Val loss 0.06308160722255707, Val error 0.0199\n2020-08-14 13:18:04,386 Epoch 243 finished. Val loss 0.057617396116256714, Val error 0.0188\n2020-08-14 13:18:12,286 Epoch 244 finished. Val loss 0.07476384937763214, Val error 0.0238\n2020-08-14 13:18:20,071 Epoch 245 finished. Val loss 0.06709153950214386, Val error 0.0218\n2020-08-14 13:18:28,043 Epoch 246 finished. Val loss 0.06571953743696213, Val error 0.0204\n2020-08-14 13:18:35,932 Epoch 247 finished. Val loss 0.06564104557037354, Val error 0.0194\n2020-08-14 13:18:43,831 Epoch 248 finished. Val loss 0.06531064212322235, Val error 0.0191\n2020-08-14 13:18:51,665 Epoch 249 finished. Val loss 0.06611635535955429, Val error 0.0188\n2020-08-14 13:18:59,583 Epoch 250 finished. Val loss 0.05974495783448219, Val error 0.0176\n2020-08-14 13:19:07,415 Epoch 251 finished. Val loss 0.06580933183431625, Val error 0.0206\n2020-08-14 13:19:15,322 Epoch 252 finished. Val loss 0.06796321272850037, Val error 0.0193\n2020-08-14 13:19:23,312 Epoch 253 finished. Val loss 0.07572299242019653, Val error 0.0239\n2020-08-14 13:19:31,665 Epoch 254 finished. Val loss 0.06418604403734207, Val error 0.0198\n2020-08-14 13:19:39,614 Epoch 255 finished. Val loss 0.06244007498025894, Val error 0.0187\n2020-08-14 13:19:47,503 Epoch 256 finished. Val loss 0.061966270208358765, Val error 0.0201\n2020-08-14 13:19:55,652 Epoch 257 finished. Val loss 0.05910343676805496, Val error 0.0174\n2020-08-14 13:20:03,691 Epoch 258 finished. Val loss 0.06378983706235886, Val error 0.0196\n2020-08-14 13:20:11,608 Epoch 259 finished. Val loss 0.06052831560373306, Val error 0.0193\n2020-08-14 13:20:19,650 Epoch 260 finished. Val loss 0.06322608888149261, Val error 0.0187\n2020-08-14 13:20:27,657 Epoch 261 finished. Val loss 0.056806132197380066, Val error 0.0179\n2020-08-14 13:20:35,690 Epoch 262 finished. Val loss 0.06223723292350769, Val error 0.0197\n2020-08-14 13:20:43,536 Epoch 263 finished. Val loss 0.07219734787940979, Val error 0.0234\n2020-08-14 13:20:51,458 Epoch 264 finished. Val loss 0.06323479115962982, Val error 0.019\n2020-08-14 13:20:59,454 Epoch 265 finished. Val loss 0.06301906704902649, Val error 0.0191\n2020-08-14 13:21:07,376 Epoch 266 finished. Val loss 0.06548751145601273, Val error 0.0195\n2020-08-14 13:21:15,201 Epoch 267 finished. Val loss 0.07431188970804214, Val error 0.0224\n2020-08-14 13:21:24,354 Epoch 268 finished. Val loss 0.06531352549791336, Val error 0.0203\n2020-08-14 13:21:37,098 Epoch 269 finished. Val loss 0.06267481297254562, Val error 0.019\n2020-08-14 13:21:47,025 Epoch 270 finished. Val loss 0.06389833241701126, Val error 0.0198\n2020-08-14 13:21:55,100 Epoch 271 finished. Val loss 0.06616784632205963, Val error 0.021\n2020-08-14 13:22:04,267 Epoch 272 finished. Val loss 0.06633590161800385, Val error 0.0193\n2020-08-14 13:22:16,559 Epoch 273 finished. Val loss 0.06617173552513123, Val error 0.0214\n2020-08-14 13:22:27,203 Epoch 274 finished. Val loss 0.06516409665346146, Val error 0.0195\n2020-08-14 13:22:37,886 Epoch 275 finished. Val loss 0.07652105391025543, Val error 0.0229\n2020-08-14 13:22:48,631 Epoch 276 finished. Val loss 0.06575474143028259, Val error 0.0206\n2020-08-14 13:23:00,992 Epoch 277 finished. Val loss 0.06444866210222244, Val error 0.0188\n2020-08-14 13:23:10,759 Epoch 278 finished. Val loss 0.06895843148231506, Val error 0.0219\n2020-08-14 13:23:23,879 Epoch 279 finished. Val loss 0.06266100704669952, Val error 0.0189\n2020-08-14 13:23:32,653 Epoch 280 finished. Val loss 0.07044032961130142, Val error 0.0231\n2020-08-14 13:23:44,695 Epoch 281 finished. Val loss 0.06679517775774002, Val error 0.0199\n2020-08-14 13:23:55,540 Epoch 282 finished. Val loss 0.0592619925737381, Val error 0.0179\n2020-08-14 13:24:06,912 Epoch 283 finished. Val loss 0.06979358196258545, Val error 0.0216\n2020-08-14 13:24:18,814 Epoch 284 finished. Val loss 0.06488043814897537, Val error 0.0196\n2020-08-14 13:24:31,642 Epoch 285 finished. Val loss 0.07208699733018875, Val error 0.0209\n2020-08-14 13:24:46,519 Epoch 286 finished. Val loss 0.06653807312250137, Val error 0.0194\n2020-08-14 13:25:00,082 Epoch 287 finished. Val loss 0.06629950553178787, Val error 0.0204\n2020-08-14 13:25:08,153 Epoch 288 finished. Val loss 0.06557826697826385, Val error 0.0201\n2020-08-14 13:25:19,101 Epoch 289 finished. Val loss 0.06983668357133865, Val error 0.0231\n2020-08-14 13:25:28,913 Epoch 290 finished. Val loss 0.06448621302843094, Val error 0.0186\n2020-08-14 13:25:40,962 Epoch 291 finished. Val loss 0.0665682926774025, Val error 0.0189\n2020-08-14 13:25:50,970 Epoch 292 finished. Val loss 0.07014413177967072, Val error 0.0208\n2020-08-14 13:26:03,786 Epoch 293 finished. Val loss 0.0674782246351242, Val error 0.0211\n2020-08-14 13:26:14,703 Epoch 294 finished. Val loss 0.06434899568557739, Val error 0.0192\n2020-08-14 13:26:22,794 Epoch 295 finished. Val loss 0.06596119701862335, Val error 0.0202\n2020-08-14 13:26:32,503 Epoch 296 finished. Val loss 0.06573452055454254, Val error 0.0198\n2020-08-14 13:26:42,630 Epoch 297 finished. Val loss 0.06097352132201195, Val error 0.0191\n2020-08-14 13:26:54,633 Epoch 298 finished. Val loss 0.0647987574338913, Val error 0.0192\n2020-08-14 13:27:03,809 Epoch 299 finished. Val loss 0.0604061633348465, Val error 0.0186\n"
    }
   ],
   "source": [
    "trainer.train(n_epoch=n_epoch, burn_in_epochs=burn_in_epochs, resample_prior_until=resample_prior_until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100\n"
    }
   ],
   "source": [
    "weights_set = trainer.weight_set_samples[-(n_epoch - resample_prior_until) // save_freq:]\n",
    "\n",
    "print(len(weights_set))\n",
    "\n",
    "Path('../saved_samples', 'mnist_weights').mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(weights_set, Path('../saved_samples', 'mnist_weights', 'weights.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_set = weights_set[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10"
     },
     "metadata": {},
     "execution_count": 354
    }
   ],
   "source": [
    "len(weights_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict_to_vec(state_dict):\n",
    "    return torch.cat([w_i.view(-1) for w_i in state_dict.values()])\n",
    "\n",
    "squeezed_weights = [state_dict_to_vec(w) for w in weights_set]\n",
    "\n",
    "models = [MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim) for w in weights_set]\n",
    "for w, model in zip(weights_set, models):\n",
    "    model.load_state_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_with_priors = trainer.optimizer\n",
    "\n",
    "priors = {}\n",
    "group_params = opt_with_priors.param_groups[0]['params']\n",
    "for (n, _), p in zip(model.named_parameters(), group_params):  \n",
    "    state = opt_with_priors.state[p]  \n",
    "    priors[n] = state['weight_decay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x, model):\n",
    "    return F.softmax(model(x), dim=-1)\n",
    "\n",
    "def get_binary_prediction(x, model, classes):\n",
    "    assert len(classes) == 2\n",
    "    return F.softmax(model(x)[..., classes], dim=-1)[..., -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(x, y, model):\n",
    "    y_hat = model(x)\n",
    "    log_likelihood = -F.cross_entropy(y_hat, y, reduction='mean')\n",
    "    return log_likelihood\n",
    "\n",
    "def compute_mc_estimate(function: callable, models, x: torch.tensor):\n",
    "    res = 0.0\n",
    "    for model in models:\n",
    "        res += function(x, model)\n",
    "    return res / len(models)\n",
    "\n",
    "def compute_naive_variance(function:callable, control_variate: callable, models, x: torch.tensor):\n",
    "    sample_mean = compute_mc_estimate(lambda x_, model: function(x_, model) - control_variate(x_, model), models, x)\n",
    "    v = 0\n",
    "    v_no_cv = 0\n",
    "    for model in models:\n",
    "        v += (function(x, model) - control_variate(x, model) - sample_mean)**2 / (len(models) - 1)\n",
    "        v_no_cv += (function(x, model) - compute_mc_estimate(function, models, x))**2 / (len(models) - 1)\n",
    "    return v, v_no_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tricky_divergence(model):\n",
    "    div = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            div += p.grad.sum()\n",
    "\n",
    "    return div\n",
    "\n",
    "def stein_control_variate(psy_model, model, train_x, train_y, x_new, priors, N_train):\n",
    "    model.zero_grad()\n",
    "    log_likelihood = compute_log_likelihood(train_x, train_y, model) * N_train\n",
    "    log_likelihood.backward()\n",
    "    \n",
    "    ncv_value = 0\n",
    "    psy_value = psy_model(state_dict_to_vec(model.state_dict()), x_new).view(1)\n",
    "\n",
    "    psy_value.backward(retain_graph=True)\n",
    "    psy_div = compute_tricky_divergence(psy_model)\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            d_p = p.grad.data\n",
    "            d_p.add_(p.data, alpha=-priors[n])\n",
    "\n",
    "            ncv_value += d_p.sum()\n",
    "\n",
    "    ncv_value = torch.cat([ncv_value.view(1)]*psy_value.shape[0], dim=0)\n",
    "\n",
    "    ncv_value *= psy_value\n",
    "    ncv_value += psy_div\n",
    "\n",
    "    return ncv_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PsyLinear(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        return  self.layer(weights)\n",
    "\n",
    "class PsyMLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.LeakyReLU()]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        layers.append(nn.Linear(width, 1, bias=False))\n",
    "        #layers.append(nn.Tanh())\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "        #for p in self.parameters():\n",
    "        #    torch.nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        return self.block(weights)\n",
    "\n",
    "class PsyDoubleMLP(nn.Module):\n",
    "    def __init__(self, input_dim1, width, depth1, input_dim2, depth2):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        layers1 = [nn.Linear(input_dim1, width), nn.ReLU()]\n",
    "        for i in range(depth1 - 1):\n",
    "            layers1.append(nn.Linear(width, width))\n",
    "            layers1.append(nn.ReLU())\n",
    "        #layers.append(nn.Tanh())\n",
    "\n",
    "        self.block1 = nn.Sequential(*layers1)\n",
    "\n",
    "        layers2 = [nn.Linear(input_dim2, width), nn.ReLU()]\n",
    "        for i in range(depth2 - 1):\n",
    "            layers2.append(nn.Linear(width, width))\n",
    "            layers2.append(nn.ReLU())\n",
    "        #layers.append(nn.Tanh())\n",
    "\n",
    "        self.block2 = nn.Sequential(*layers2)\n",
    "\n",
    "        self.final = nn.Linear(width, 1, bias=False)\n",
    "\n",
    "        for n, p in self.named_parameters():\n",
    "           if p.ndim >= 2:\n",
    "               torch.nn.init.xavier_uniform(p)\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.final(self.block1(weights) + self.block2(x))#.view(weights.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_hidden = 150\n",
    "psy_depth1 = 3\n",
    "psy_depth2 = 2\n",
    "n_iter = 100\n",
    "psy_lr = 1e-2\n",
    "psy_input1 = squeezed_weights[0].shape[0]\n",
    "N_train = batch_size * len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = next(iter(valloader))\n",
    "#x_new = x_new\n",
    "train_x, train_y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_model = PsyDoubleMLP(psy_input1, psy_hidden, psy_depth1, 784, psy_depth2)\n",
    "psy_model.to(device)\n",
    "\n",
    "neural_control_variate = lambda x, model : stein_control_variate(psy_model, model, train_x, train_y, x, priors, N_train)\n",
    "\n",
    "ncv_optimizer = torch.optim.Adam(psy_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "r([0.0002], grad_fn=<AddBackward0>)\ntensor([51011036.], grad_fn=<AddBackward0>) tensor([0.0060], grad_fn=<AddBackward0>)\ntensor([1920822.2500], grad_fn=<AddBackward0>) tensor([0.0189], grad_fn=<AddBackward0>)\ntensor([2603486.2500], grad_fn=<AddBackward0>) tensor([5.0434e-07], grad_fn=<AddBackward0>)\ntensor([27166830.], grad_fn=<AddBackward0>) tensor([0.0006], grad_fn=<AddBackward0>)\ntensor([4103902.], grad_fn=<AddBackward0>) tensor([2.3252e-10], grad_fn=<AddBackward0>)\ntensor([62288208.], grad_fn=<AddBackward0>) tensor([0.0021], grad_fn=<AddBackward0>)\ntensor([7076162.], grad_fn=<AddBackward0>) tensor([0.0071], grad_fn=<AddBackward0>)\ntensor([37799776.], grad_fn=<AddBackward0>) tensor([0.0421], grad_fn=<AddBackward0>)\ntensor([16738382.], grad_fn=<AddBackward0>) tensor([0.0093], grad_fn=<AddBackward0>)\ntensor([1.1955e+08], grad_fn=<AddBackward0>) tensor([0.0446], grad_fn=<AddBackward0>)\ntensor([1326555.2500], grad_fn=<AddBackward0>) tensor([0.0588], grad_fn=<AddBackward0>)\ntensor([96903008.], grad_fn=<AddBackward0>) tensor([2.7137e-09], grad_fn=<AddBackward0>)\ntensor([21413938.], grad_fn=<AddBackward0>) tensor([0.0405], grad_fn=<AddBackward0>)\ntensor([81353520.], grad_fn=<AddBackward0>) tensor([1.4346e-08], grad_fn=<AddBackward0>)\ntensor([10869684.], grad_fn=<AddBackward0>) tensor([0.0020], grad_fn=<AddBackward0>)\ntensor([41020032.], grad_fn=<AddBackward0>) tensor([5.4540e-06], grad_fn=<AddBackward0>)\ntensor([957306.6875], grad_fn=<AddBackward0>) tensor([0.0750], grad_fn=<AddBackward0>)\ntensor([17032736.], grad_fn=<AddBackward0>) tensor([0.0115], grad_fn=<AddBackward0>)\ntensor([32245856.], grad_fn=<AddBackward0>) tensor([0.0526], grad_fn=<AddBackward0>)\ntensor([46062236.], grad_fn=<AddBackward0>) tensor([0.0172], grad_fn=<AddBackward0>)\ntensor([48370136.], grad_fn=<AddBackward0>) tensor([0.0157], grad_fn=<AddBackward0>)\ntensor([45336152.], grad_fn=<AddBackward0>) tensor([0.0233], grad_fn=<AddBackward0>)\ntensor([21659996.], grad_fn=<AddBackward0>) tensor([0.0012], grad_fn=<AddBackward0>)\ntensor([1849818.6250], grad_fn=<AddBackward0>) tensor([0.0255], grad_fn=<AddBackward0>)\ntensor([34928232.], grad_fn=<AddBackward0>) tensor([0.0020], grad_fn=<AddBackward0>)\ntensor([7531872.5000], grad_fn=<AddBackward0>) tensor([0.0001], grad_fn=<AddBackward0>)\ntensor([1939149.7500], grad_fn=<AddBackward0>) tensor([1.2204e-07], grad_fn=<AddBackward0>)\ntensor([21470308.], grad_fn=<AddBackward0>) tensor([0.0768], grad_fn=<AddBackward0>)\ntensor([3248810.5000], grad_fn=<AddBackward0>) tensor([0.0025], grad_fn=<AddBackward0>)\ntensor([567940.], grad_fn=<AddBackward0>) tensor([0.0440], grad_fn=<AddBackward0>)\ntensor([34071352.], grad_fn=<AddBackward0>) tensor([0.0127], grad_fn=<AddBackward0>)\ntensor([540882.5625], grad_fn=<AddBackward0>) tensor([0.0024], grad_fn=<AddBackward0>)\ntensor([3509175.], grad_fn=<AddBackward0>) tensor([3.7782e-07], grad_fn=<AddBackward0>)\ntensor([5267044.], grad_fn=<AddBackward0>) tensor([7.0015e-08], grad_fn=<AddBackward0>)\ntensor([155566.9688], grad_fn=<AddBackward0>) tensor([9.8006e-05], grad_fn=<AddBackward0>)\ntensor([32580656.], grad_fn=<AddBackward0>) tensor([1.9871e-06], grad_fn=<AddBackward0>)\ntensor([990837.8750], grad_fn=<AddBackward0>) tensor([0.0028], grad_fn=<AddBackward0>)\ntensor([23405616.], grad_fn=<AddBackward0>) tensor([0.0061], grad_fn=<AddBackward0>)\ntensor([43481976.], grad_fn=<AddBackward0>) tensor([0.0361], grad_fn=<AddBackward0>)\ntensor([67757392.], grad_fn=<AddBackward0>) tensor([0.0236], grad_fn=<AddBackward0>)\ntensor([2335429.5000], grad_fn=<AddBackward0>) tensor([1.8834e-08], grad_fn=<AddBackward0>)\ntensor([12436571.], grad_fn=<AddBackward0>) tensor([0.0573], grad_fn=<AddBackward0>)\ntensor([3099677.5000], grad_fn=<AddBackward0>) tensor([0.0430], grad_fn=<AddBackward0>)\ntensor([248769.6562], grad_fn=<AddBackward0>) tensor([7.9327e-06], grad_fn=<AddBackward0>)\ntensor([2965814.5000], grad_fn=<AddBackward0>) tensor([2.1820e-09], grad_fn=<AddBackward0>)\ntensor([18061742.], grad_fn=<AddBackward0>) tensor([4.2231e-07], grad_fn=<AddBackward0>)\ntensor([1073259.6250], grad_fn=<AddBackward0>) tensor([0.0043], grad_fn=<AddBackward0>)\ntensor([2992354.5000], grad_fn=<AddBackward0>) tensor([0.0532], grad_fn=<AddBackward0>)\ntensor([2.2072e+08], grad_fn=<AddBackward0>) tensor([0.0120], grad_fn=<AddBackward0>)\ntensor([1516184.], grad_fn=<AddBackward0>) tensor([2.8359e-10], grad_fn=<AddBackward0>)\ntensor([9.3717e+08], grad_fn=<AddBackward0>) tensor([0.0125], grad_fn=<AddBackward0>)\ntensor([417074.6875], grad_fn=<AddBackward0>) tensor([0.0199], grad_fn=<AddBackward0>)\ntensor([11559762.], grad_fn=<AddBackward0>) tensor([0.0846], grad_fn=<AddBackward0>)\ntensor([43560696.], grad_fn=<AddBackward0>) tensor([0.0001], grad_fn=<AddBackward0>)\ntensor([1634611.8750], grad_fn=<AddBackward0>) tensor([0.0067], grad_fn=<AddBackward0>)\ntensor([7540985.], grad_fn=<AddBackward0>) tensor([0.0290], grad_fn=<AddBackward0>)\ntensor([8805032.], grad_fn=<AddBackward0>) tensor([1.4229e-06], grad_fn=<AddBackward0>)\ntensor([11329276.], grad_fn=<AddBackward0>) tensor([9.0135e-07], grad_fn=<AddBackward0>)\ntensor([622409.0625], grad_fn=<AddBackward0>) tensor([0.0032], grad_fn=<AddBackward0>)\ntensor([9565526.], grad_fn=<AddBackward0>) tensor([0.0170], grad_fn=<AddBackward0>)\ntensor([3973602.7500], grad_fn=<AddBackward0>) tensor([0.0027], grad_fn=<AddBackward0>)\ntensor([621910.2500], grad_fn=<AddBackward0>) tensor([0.0024], grad_fn=<AddBackward0>)\ntensor([4220285.5000], grad_fn=<AddBackward0>) tensor([0.0005], grad_fn=<AddBackward0>)\ntensor([607140.2500], grad_fn=<AddBackward0>) tensor([3.5955e-06], grad_fn=<AddBackward0>)\ntensor([4390521.], grad_fn=<AddBackward0>) tensor([0.0202], grad_fn=<AddBackward0>)\ntensor([2690965.2500], grad_fn=<AddBackward0>) tensor([0.0242], grad_fn=<AddBackward0>)\ntensor([26574080.], grad_fn=<AddBackward0>) tensor([0.0423], grad_fn=<AddBackward0>)\ntensor([8192339.5000], grad_fn=<AddBackward0>) tensor([7.5480e-05], grad_fn=<AddBackward0>)\ntensor([2814066.7500], grad_fn=<AddBackward0>) tensor([3.6013e-06], grad_fn=<AddBackward0>)\ntensor([26130586.], grad_fn=<AddBackward0>) tensor([0.0014], grad_fn=<AddBackward0>)\ntensor([13229061.], grad_fn=<AddBackward0>) tensor([0.0224], grad_fn=<AddBackward0>)\ntensor([104472.8281], grad_fn=<AddBackward0>) tensor([8.1713e-08], grad_fn=<AddBackward0>)\ntensor([14763332.], grad_fn=<AddBackward0>) tensor([0.0023], grad_fn=<AddBackward0>)\ntensor([2108327.7500], grad_fn=<AddBackward0>) tensor([0.0239], grad_fn=<AddBackward0>)\ntensor([360057.6875], grad_fn=<AddBackward0>) tensor([0.0002], grad_fn=<AddBackward0>)\ntensor([1926717.7500], grad_fn=<AddBackward0>) tensor([0.0086], grad_fn=<AddBackward0>)\ntensor([1814848.8750], grad_fn=<AddBackward0>) tensor([0.0056], grad_fn=<AddBackward0>)\ntensor([488174.7812], grad_fn=<AddBackward0>) tensor([0.0306], grad_fn=<AddBackward0>)\ntensor([3430033.], grad_fn=<AddBackward0>) tensor([1.2654e-06], grad_fn=<AddBackward0>)\ntensor([980497.6875], grad_fn=<AddBackward0>) tensor([1.0466e-05], grad_fn=<AddBackward0>)\ntensor([9101687.], grad_fn=<AddBackward0>) tensor([0.0060], grad_fn=<AddBackward0>)\ntensor([7844179.], grad_fn=<AddBackward0>) tensor([0.0005], grad_fn=<AddBackward0>)\ntensor([3556402.5000], grad_fn=<AddBackward0>) tensor([0.0098], grad_fn=<AddBackward0>)\ntensor([139390.0938], grad_fn=<AddBackward0>) tensor([4.0867e-08], grad_fn=<AddBackward0>)\ntensor([66569176.], grad_fn=<AddBackward0>) tensor([0.0053], grad_fn=<AddBackward0>)\ntensor([67332016.], grad_fn=<AddBackward0>) tensor([0.0005], grad_fn=<AddBackward0>)\ntensor([2027342.], grad_fn=<AddBackward0>) tensor([0.0374], grad_fn=<AddBackward0>)\ntensor([57392056.], grad_fn=<AddBackward0>) tensor([0.0005], grad_fn=<AddBackward0>)\ntensor([3522123.2500], grad_fn=<AddBackward0>) tensor([0.0024], grad_fn=<AddBackward0>)\ntensor([1737731.], grad_fn=<AddBackward0>) tensor([0.0031], grad_fn=<AddBackward0>)\ntensor([3115779.], grad_fn=<AddBackward0>) tensor([0.0099], grad_fn=<AddBackward0>)\ntensor([17468104.], grad_fn=<AddBackward0>) tensor([0.0021], grad_fn=<AddBackward0>)\ntensor([13733570.], grad_fn=<AddBackward0>) tensor([0.0002], grad_fn=<AddBackward0>)\ntensor([3656718.5000], grad_fn=<AddBackward0>) tensor([1.8003e-06], grad_fn=<AddBackward0>)\ntensor([174596.3906], grad_fn=<AddBackward0>) tensor([0.0024], grad_fn=<AddBackward0>)\ntensor([277081.0625], grad_fn=<AddBackward0>) tensor([3.6234e-05], grad_fn=<AddBackward0>)\ntensor([12905692.], grad_fn=<AddBackward0>) tensor([0.0176], grad_fn=<AddBackward0>)\ntensor([342873.9375], grad_fn=<AddBackward0>) tensor([0.0132], grad_fn=<AddBackward0>)\ntensor([2.3245e+08], grad_fn=<AddBackward0>) tensor([0.0352], grad_fn=<AddBackward0>)\ntensor([288398.1875], grad_fn=<AddBackward0>) tensor([0.0171], grad_fn=<AddBackward0>)\ntensor([44854868.], grad_fn=<AddBackward0>) tensor([0.0098], grad_fn=<AddBackward0>)\ntensor([2504368.2500], grad_fn=<AddBackward0>) tensor([2.3486e-05], grad_fn=<AddBackward0>)\ntensor([1334279.2500], grad_fn=<AddBackward0>) tensor([0.0173], grad_fn=<AddBackward0>)\ntensor([1103254.5000], grad_fn=<AddBackward0>) tensor([0.0130], grad_fn=<AddBackward0>)\ntensor([8572372.], grad_fn=<AddBackward0>) tensor([0.0012], grad_fn=<AddBackward0>)\ntensor([15230401.], grad_fn=<AddBackward0>) tensor([0.0446], grad_fn=<AddBackward0>)\ntensor([32172206.], grad_fn=<AddBackward0>) tensor([0.0053], grad_fn=<AddBackward0>)\ntensor([13943807.], grad_fn=<AddBackward0>) tensor([0.0395], grad_fn=<AddBackward0>)\ntensor([15099161.], grad_fn=<AddBackward0>) tensor([1.6230e-10], grad_fn=<AddBackward0>)\ntensor([268654.7500], grad_fn=<AddBackward0>) tensor([0.0945], grad_fn=<AddBackward0>)\ntensor([1137664.5000], grad_fn=<AddBackward0>) tensor([7.5400e-10], grad_fn=<AddBackward0>)\ntensor([1808321.3750], grad_fn=<AddBackward0>) tensor([0.0027], grad_fn=<AddBackward0>)\ntensor([4394901.], grad_fn=<AddBackward0>) tensor([8.5934e-06], grad_fn=<AddBackward0>)\ntensor([14116229.], grad_fn=<AddBackward0>) tensor([8.7369e-08], grad_fn=<AddBackward0>)\ntensor([83831.6641], grad_fn=<AddBackward0>) tensor([7.0254e-06], grad_fn=<AddBackward0>)\ntensor([4193948.5000], grad_fn=<AddBackward0>) tensor([0.0477], grad_fn=<AddBackward0>)\ntensor([1813673.7500], grad_fn=<AddBackward0>) tensor([0.0141], grad_fn=<AddBackward0>)\ntensor([575759.5000], grad_fn=<AddBackward0>) tensor([0.0291], grad_fn=<AddBackward0>)\ntensor([2133583.7500], grad_fn=<AddBackward0>) tensor([0.0279], grad_fn=<AddBackward0>)\ntensor([1529410.5000], grad_fn=<AddBackward0>) tensor([0.0027], grad_fn=<AddBackward0>)\ntensor([20063768.], grad_fn=<AddBackward0>) tensor([0.0766], grad_fn=<AddBackward0>)\ntensor([3541321.], grad_fn=<AddBackward0>) tensor([0.0010], grad_fn=<AddBackward0>)\ntensor([236017.7188], grad_fn=<AddBackward0>) tensor([0.0460], grad_fn=<AddBackward0>)\ntensor([259690.1250], grad_fn=<AddBackward0>) tensor([3.4866e-10], grad_fn=<AddBackward0>)\ntensor([2668749.], grad_fn=<AddBackward0>) tensor([0.0183], grad_fn=<AddBackward0>)\ntensor([2895322.], grad_fn=<AddBackward0>) tensor([0.0006], grad_fn=<AddBackward0>)\ntensor([17034792.], grad_fn=<AddBackward0>) tensor([0.0584], grad_fn=<AddBackward0>)\ntensor([37369068.], grad_fn=<AddBackward0>) tensor([0.0190], grad_fn=<AddBackward0>)\ntensor([47693384.], grad_fn=<AddBackward0>) tensor([6.8853e-06], grad_fn=<AddBackward0>)\ntensor([10581172.], grad_fn=<AddBackward0>) tensor([0.0674], grad_fn=<AddBackward0>)\ntensor([35299440.], grad_fn=<AddBackward0>) tensor([0.0255], grad_fn=<AddBackward0>)\ntensor([2604208.2500], grad_fn=<AddBackward0>) tensor([0.0559], grad_fn=<AddBackward0>)\ntensor([33002394.], grad_fn=<AddBackward0>) tensor([0.0684], grad_fn=<AddBackward0>)\ntensor([1932417.6250], grad_fn=<AddBackward0>) tensor([1.3729e-11], grad_fn=<AddBackward0>)\ntensor([2887132.2500], grad_fn=<AddBackward0>) tensor([1.2010e-08], grad_fn=<AddBackward0>)\ntensor([393784.8125], grad_fn=<AddBackward0>) tensor([0.0642], grad_fn=<AddBackward0>)\ntensor([7578203.], grad_fn=<AddBackward0>) tensor([8.5518e-12], grad_fn=<AddBackward0>)\ntensor([6445193.], grad_fn=<AddBackward0>) tensor([0.0001], grad_fn=<AddBackward0>)\ntensor([6365681.5000], grad_fn=<AddBackward0>) tensor([0.0138], grad_fn=<AddBackward0>)\ntensor([19818760.], grad_fn=<AddBackward0>) tensor([7.4103e-06], grad_fn=<AddBackward0>)\ntensor([2393534.5000], grad_fn=<AddBackward0>) tensor([0.0083], grad_fn=<AddBackward0>)\ntensor([2138330.], grad_fn=<AddBackward0>) tensor([0.0025], grad_fn=<AddBackward0>)\ntensor([721307.1875], grad_fn=<AddBackward0>) tensor([0.0011], grad_fn=<AddBackward0>)\ntensor([4275574.], grad_fn=<AddBackward0>) tensor([9.1344e-13], grad_fn=<AddBackward0>)\ntensor([8209494.], grad_fn=<AddBackward0>) tensor([0.0076], grad_fn=<AddBackward0>)\ntensor([1311337.2500], grad_fn=<AddBackward0>) tensor([0.0047], grad_fn=<AddBackward0>)\ntensor([359776.6562], grad_fn=<AddBackward0>) tensor([4.6975e-10], grad_fn=<AddBackward0>)\ntensor([6936166.], grad_fn=<AddBackward0>) tensor([0.0073], grad_fn=<AddBackward0>)\ntensor([1532485.5000], grad_fn=<AddBackward0>) tensor([0.0003], grad_fn=<AddBackward0>)\ntensor([773417.8125], grad_fn=<AddBackward0>) tensor([0.0378], grad_fn=<AddBackward0>)\ntensor([1285197.5000], grad_fn=<AddBackward0>) tensor([0.0005], grad_fn=<AddBackward0>)\ntensor([546331.], grad_fn=<AddBackward0>) tensor([0.0091], grad_fn=<AddBackward0>)\ntensor([3330029.2500], grad_fn=<AddBackward0>) tensor([2.2765e-06], grad_fn=<AddBackward0>)\ntensor([9907126.], grad_fn=<AddBackward0>) tensor([4.7848e-08], grad_fn=<AddBackward0>)\ntensor([39252416.], grad_fn=<AddBackward0>) tensor([1.9407e-05], grad_fn=<AddBackward0>)\ntensor([2733883.2500], grad_fn=<AddBackward0>) tensor([4.2210e-05], grad_fn=<AddBackward0>)\ntensor([8367155.5000], grad_fn=<AddBackward0>) tensor([0.0012], grad_fn=<AddBackward0>)\ntensor([306291.8125], grad_fn=<AddBackward0>) tensor([0.0254], grad_fn=<AddBackward0>)\ntensor([1106567.2500], grad_fn=<AddBackward0>) tensor([0.0395], grad_fn=<AddBackward0>)\ntensor([3553897.], grad_fn=<AddBackward0>) tensor([0.0236], grad_fn=<AddBackward0>)\ntensor([3717204.], grad_fn=<AddBackward0>) tensor([0.0794], grad_fn=<AddBackward0>)\ntensor([254694.7500], grad_fn=<AddBackward0>) tensor([0.0230], grad_fn=<AddBackward0>)\ntensor([9217976.], grad_fn=<AddBackward0>) tensor([0.0720], grad_fn=<AddBackward0>)\ntensor([12918567.], grad_fn=<AddBackward0>) tensor([8.9984e-07], grad_fn=<AddBackward0>)\ntensor([22337350.], grad_fn=<AddBackward0>) tensor([0.0830], grad_fn=<AddBackward0>)\ntensor([411134.9062], grad_fn=<AddBackward0>) tensor([0.0020], grad_fn=<AddBackward0>)\ntensor([4273598.], grad_fn=<AddBackward0>) tensor([0.0269], grad_fn=<AddBackward0>)\ntensor([2902170.2500], grad_fn=<AddBackward0>) tensor([3.7047e-09], grad_fn=<AddBackward0>)\ntensor([25846680.], grad_fn=<AddBackward0>) tensor([0.0004], grad_fn=<AddBackward0>)\ntensor([14788879.], grad_fn=<AddBackward0>) tensor([3.4015e-05], grad_fn=<AddBackward0>)\ntensor([90554.2031], grad_fn=<AddBackward0>) tensor([0.0322], grad_fn=<AddBackward0>)\ntensor([7182260.], grad_fn=<AddBackward0>) tensor([0.0190], grad_fn=<AddBackward0>)\ntensor([14280120.], grad_fn=<AddBackward0>) tensor([0.0440], grad_fn=<AddBackward0>)\ntensor([66975048.], grad_fn=<AddBackward0>) tensor([0.0496], grad_fn=<AddBackward0>)\ntensor([7309844.], grad_fn=<AddBackward0>) tensor([0.0576], grad_fn=<AddBackward0>)\ntensor([3755216.], grad_fn=<AddBackward0>) tensor([0.0033], grad_fn=<AddBackward0>)\ntensor([3358256.], grad_fn=<AddBackward0>) tensor([0.0156], grad_fn=<AddBackward0>)\ntensor([848240.6250], grad_fn=<AddBackward0>) tensor([0.0092], grad_fn=<AddBackward0>)\ntensor([150641.1094], grad_fn=<AddBackward0>) tensor([3.6982e-07], grad_fn=<AddBackward0>)\ntensor([4206181.5000], grad_fn=<AddBackward0>) tensor([0.0097], grad_fn=<AddBackward0>)\ntensor([4640544.5000], grad_fn=<AddBackward0>) tensor([5.9174e-09], grad_fn=<AddBackward0>)\ntensor([22819728.], grad_fn=<AddBackward0>) tensor([2.6112e-05], grad_fn=<AddBackward0>)\ntensor([209158.6562], grad_fn=<AddBackward0>) tensor([0.0125], grad_fn=<AddBackward0>)\ntensor([4227681.], grad_fn=<AddBackward0>) tensor([0.0316], grad_fn=<AddBackward0>)\ntensor([3611605.5000], grad_fn=<AddBackward0>) tensor([0.0383], grad_fn=<AddBackward0>)\ntensor([1620937.], grad_fn=<AddBackward0>) tensor([0.0275], grad_fn=<AddBackward0>)\ntensor([24750404.], grad_fn=<AddBackward0>) tensor([7.0850e-08], grad_fn=<AddBackward0>)\ntensor([12844530.], grad_fn=<AddBackward0>) tensor([0.0153], grad_fn=<AddBackward0>)\ntensor([233376.9844], grad_fn=<AddBackward0>) tensor([3.8723e-05], grad_fn=<AddBackward0>)\ntensor([26904316.], grad_fn=<AddBackward0>) tensor([0.0040], grad_fn=<AddBackward0>)\ntensor([714726.3750], grad_fn=<AddBackward0>) tensor([0.0190], grad_fn=<AddBackward0>)\ntensor([100168.5547], grad_fn=<AddBackward0>) tensor([0.0064], grad_fn=<AddBackward0>)\ntensor([282359.0312], grad_fn=<AddBackward0>) tensor([0.0002], grad_fn=<AddBackward0>)\ntensor([77221584.], grad_fn=<AddBackward0>) tensor([0.0351], grad_fn=<AddBackward0>)\ntensor([3035330.5000], grad_fn=<AddBackward0>) tensor([3.6699e-11], grad_fn=<AddBackward0>)\ntensor([3782159.5000], grad_fn=<AddBackward0>) tensor([0.0228], grad_fn=<AddBackward0>)\ntensor([4613732.5000], grad_fn=<AddBackward0>) tensor([0.0517], grad_fn=<AddBackward0>)\ntensor([12741206.], grad_fn=<AddBackward0>) tensor([6.6986e-13], grad_fn=<AddBackward0>)\ntensor([7390737.], grad_fn=<AddBackward0>) tensor([1.8957e-09], grad_fn=<AddBackward0>)\ntensor([1550940.5000], grad_fn=<AddBackward0>) tensor([2.4405e-08], grad_fn=<AddBackward0>)\ntensor([10086199.], grad_fn=<AddBackward0>) tensor([8.1937e-06], grad_fn=<AddBackward0>)\ntensor([25336768.], grad_fn=<AddBackward0>) tensor([0.0428], grad_fn=<AddBackward0>)\ntensor([1696983.6250], grad_fn=<AddBackward0>) tensor([0.0045], grad_fn=<AddBackward0>)\ntensor([3219661.2500], grad_fn=<AddBackward0>) tensor([0.0021], grad_fn=<AddBackward0>)\ntensor([8536506.], grad_fn=<AddBackward0>) tensor([0.0010], grad_fn=<AddBackward0>)\ntensor([13134.7930], grad_fn=<AddBackward0>) tensor([0.0094], grad_fn=<AddBackward0>)\ntensor([1491981.1250], grad_fn=<AddBackward0>) tensor([9.0292e-07], grad_fn=<AddBackward0>)\ntensor([1782246.5000], grad_fn=<AddBackward0>) tensor([0.0396], grad_fn=<AddBackward0>)\ntensor([546776.1250], grad_fn=<AddBackward0>) tensor([0.0262], grad_fn=<AddBackward0>)\ntensor([365515.6562], grad_fn=<AddBackward0>) tensor([0.0344], grad_fn=<AddBackward0>)\ntensor([3801288.5000], grad_fn=<AddBackward0>) tensor([0.0018], grad_fn=<AddBackward0>)\ntensor([2649602.], grad_fn=<AddBackward0>) tensor([0.0004], grad_fn=<AddBackward0>)\ntensor([4535179.5000], grad_fn=<AddBackward0>) tensor([3.1627e-07], grad_fn=<AddBackward0>)\ntensor([3671064.7500], grad_fn=<AddBackward0>) tensor([3.0315e-07], grad_fn=<AddBackward0>)\ntensor([16759208.], grad_fn=<AddBackward0>) tensor([0.0002], grad_fn=<AddBackward0>)\ntensor([974753.5625], grad_fn=<AddBackward0>) tensor([0.0001], grad_fn=<AddBackward0>)\ntensor([9292096.], grad_fn=<AddBackward0>) tensor([0.0063], grad_fn=<AddBackward0>)\ntensor([528845.8125], grad_fn=<AddBackward0>) tensor([1.8565e-05], grad_fn=<AddBackward0>)\ntensor([2342837.5000], grad_fn=<AddBackward0>) tensor([8.1983e-05], grad_fn=<AddBackward0>)\ntensor([171101.6562], grad_fn=<AddBackward0>) tensor([0.0038], grad_fn=<AddBackward0>)\ntensor([208635.7812], grad_fn=<AddBackward0>) tensor([0.0454], grad_fn=<AddBackward0>)\ntensor([158879.9375], grad_fn=<AddBackward0>) tensor([7.7644e-06], grad_fn=<AddBackward0>)\ntensor([2833983.5000], grad_fn=<AddBackward0>) tensor([0.0552], grad_fn=<AddBackward0>)\ntensor([440357.3750], grad_fn=<AddBackward0>) tensor([0.0015], grad_fn=<AddBackward0>)\ntensor([199355.5625], grad_fn=<AddBackward0>) tensor([0.0002], grad_fn=<AddBackward0>)\ntensor([1620113.7500], grad_fn=<AddBackward0>) tensor([9.7882e-05], grad_fn=<AddBackward0>)\ntensor([1233288.3750], grad_fn=<AddBackward0>) tensor([0.0036], grad_fn=<AddBackward0>)\ntensor([18224616.], grad_fn=<AddBackward0>) tensor([0.0941], grad_fn=<AddBackward0>)\ntensor([2304373.], grad_fn=<AddBackward0>) tensor([0.0109], grad_fn=<AddBackward0>)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-611-89b0791145d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmc_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_cv_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_naive_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_control_variate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_cv_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmc_variance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mncv_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "function_f = lambda x, model: get_binary_prediction(x, model, classes=[3, 5])\n",
    "\n",
    "for it in range(n_iter):\n",
    "    for x in x_new:\n",
    "        ncv_optimizer.zero_grad()\n",
    "        mc_variance, no_cv_variance = compute_naive_variance(function_f, neural_control_variate, models, x)\n",
    "        print(mc_variance, no_cv_variance)\n",
    "        mc_variance.backward()\n",
    "        ncv_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "block1.0.weight Parameter containing:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\nblock1.0.bias Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.], requires_grad=True)\nblock1.2.weight Parameter containing:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\nblock1.2.bias Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.], requires_grad=True)\nblock1.4.weight Parameter containing:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\nblock1.4.bias Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.], requires_grad=True)\nblock2.0.weight Parameter containing:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\nblock2.0.bias Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.], requires_grad=True)\nblock2.2.weight Parameter containing:\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\nblock2.2.bias Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.], requires_grad=True)\nfinal.weight Parameter containing:\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.]], requires_grad=True)\n"
    }
   ],
   "source": [
    "for n, p in psy_model.named_parameters():\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.], grad_fn=<SqueezeBackward3>)"
     },
     "metadata": {},
     "execution_count": 501
    }
   ],
   "source": [
    "psy_model(squeezed_weights[0], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.], grad_fn=<SqueezeBackward3>)"
     },
     "metadata": {},
     "execution_count": 508
    }
   ],
   "source": [
    "psy_model(squeezed_weights[7], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}