{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from control_variates.model import MLP\n",
    "from control_variates.optim import LangevinSGD as SGLD, ScaleAdaSGHMC as H_SA_SGHMC\n",
    "from mnist_utils import load_mnist_dataset\n",
    "from control_variates.trainer import BNNTrainer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from pathlib import Path\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "input_dim = 784\n",
    "width = 100\n",
    "depth = 2\n",
    "output_dim = 2\n",
    "lr = 1e-3\n",
    "n_epoch = 200\n",
    "alpha0, beta0 = 10, 10\n",
    "resample_prior_every = 15\n",
    "resample_momentum_every = 50\n",
    "burn_in_epochs = 20\n",
    "save_freq = 4\n",
    "resample_prior_until = 100\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Берем два класса из МНИСТа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Path('../data', 'mnist').mkdir(exist_ok=True, parents=True)\n",
    "trainloader, valloader = load_mnist_dataset(Path('../data', 'mnist'), batch_size, [3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_variates.model import LogRegression\n",
    "model = LogRegression(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGLD(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "optimizer = H_SA_SGHMC(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_func(y_hat, y):\n",
    "    nll = F.cross_entropy(y_hat, y, reduction='sum')\n",
    "    return nll\n",
    "\n",
    "def err_func(y_hat, y):\n",
    "    err = y_hat.argmax(-1).ne(y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BNNTrainer(model, optimizer, nll_func, err_func, trainloader, valloader, device=device, \n",
    "    resample_prior_every=resample_prior_every,\n",
    "    resample_momentum_every=resample_momentum_every,\n",
    "    save_freq=save_freq,\n",
    "    batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "-08-15 11:50:23,809 Epoch 7 finished. Val loss 0.09057198464870453, Val error 0.033123028391167195\n2020-08-15 11:50:26,516 Epoch 8 finished. Val loss 0.09757083654403687, Val error 0.03470031545741325\n2020-08-15 11:50:29,020 Epoch 9 finished. Val loss 0.09389007091522217, Val error 0.03785488958990536\n2020-08-15 11:50:31,001 Epoch 10 finished. Val loss 0.08884655684232712, Val error 0.03470031545741325\n2020-08-15 11:50:32,738 Epoch 11 finished. Val loss 0.09444264322519302, Val error 0.03890641430073607\n2020-08-15 11:50:34,488 Epoch 12 finished. Val loss 0.09036420285701752, Val error 0.03785488958990536\n2020-08-15 11:50:36,218 Epoch 13 finished. Val loss 0.09465666115283966, Val error 0.035226077812828605\n2020-08-15 11:50:37,950 Epoch 14 finished. Val loss 0.09984655678272247, Val error 0.03627760252365931\n2020-08-15 11:50:39,702 Epoch 15 finished. Val loss 0.09045618772506714, Val error 0.033648790746582544\n2020-08-15 11:50:41,460 Epoch 16 finished. Val loss 0.09838180989027023, Val error 0.03943217665615142\n2020-08-15 11:50:43,215 Epoch 17 finished. Val loss 0.09147679060697556, Val error 0.03470031545741325\n2020-08-15 11:50:45,038 Epoch 18 finished. Val loss 0.09227309376001358, Val error 0.03470031545741325\n2020-08-15 11:50:46,864 Epoch 19 finished. Val loss 0.08983226865530014, Val error 0.03680336487907466\n2020-08-15 11:50:49,715 Epoch 20 finished. Val loss 0.09215318411588669, Val error 0.03785488958990536\n2020-08-15 11:50:51,835 Epoch 21 finished. Val loss 0.09298226237297058, Val error 0.03470031545741325\n2020-08-15 11:50:54,356 Epoch 22 finished. Val loss 0.0907476618885994, Val error 0.033648790746582544\n2020-08-15 11:50:56,398 Epoch 23 finished. Val loss 0.08730611950159073, Val error 0.0341745531019979\n2020-08-15 11:50:58,545 Epoch 24 finished. Val loss 0.09029154479503632, Val error 0.033123028391167195\n2020-08-15 11:51:01,350 Epoch 25 finished. Val loss 0.09003689140081406, Val error 0.035226077812828605\n2020-08-15 11:51:03,276 Epoch 26 finished. Val loss 0.08987762033939362, Val error 0.035751840168243953\n2020-08-15 11:51:04,992 Epoch 27 finished. Val loss 0.09447503089904785, Val error 0.03470031545741325\n2020-08-15 11:51:06,890 Epoch 28 finished. Val loss 0.09044603258371353, Val error 0.03470031545741325\n2020-08-15 11:51:08,826 Epoch 29 finished. Val loss 0.0971134603023529, Val error 0.04048370136698212\n2020-08-15 11:51:10,740 Epoch 30 finished. Val loss 0.10299596190452576, Val error 0.04363827549947424\n2020-08-15 11:51:12,747 Epoch 31 finished. Val loss 0.08853987604379654, Val error 0.035226077812828605\n2020-08-15 11:51:14,417 Epoch 32 finished. Val loss 0.09404372423887253, Val error 0.03627760252365931\n2020-08-15 11:51:16,402 Epoch 33 finished. Val loss 0.08862746506929398, Val error 0.03470031545741325\n2020-08-15 11:51:18,147 Epoch 34 finished. Val loss 0.09762369841337204, Val error 0.04153522607781283\n2020-08-15 11:51:19,845 Epoch 35 finished. Val loss 0.09346598386764526, Val error 0.03838065194532071\n2020-08-15 11:51:21,766 Epoch 36 finished. Val loss 0.09550820291042328, Val error 0.03680336487907466\n2020-08-15 11:51:23,557 Epoch 37 finished. Val loss 0.09179728478193283, Val error 0.03627760252365931\n2020-08-15 11:51:25,264 Epoch 38 finished. Val loss 0.09332764893770218, Val error 0.03680336487907466\n2020-08-15 11:51:26,961 Epoch 39 finished. Val loss 0.09453289955854416, Val error 0.03838065194532071\n2020-08-15 11:51:28,669 Epoch 40 finished. Val loss 0.09198648482561111, Val error 0.03838065194532071\n2020-08-15 11:51:30,424 Epoch 41 finished. Val loss 0.08986937999725342, Val error 0.035751840168243953\n2020-08-15 11:51:32,136 Epoch 42 finished. Val loss 0.09285221993923187, Val error 0.03627760252365931\n2020-08-15 11:51:33,841 Epoch 43 finished. Val loss 0.08945121616125107, Val error 0.035226077812828605\n2020-08-15 11:51:35,551 Epoch 44 finished. Val loss 0.0929432064294815, Val error 0.033648790746582544\n2020-08-15 11:51:37,260 Epoch 45 finished. Val loss 0.09218471497297287, Val error 0.03732912723449001\n2020-08-15 11:51:38,957 Epoch 46 finished. Val loss 0.09103244543075562, Val error 0.03470031545741325\n2020-08-15 11:51:40,660 Epoch 47 finished. Val loss 0.09433471411466599, Val error 0.0341745531019979\n2020-08-15 11:51:42,417 Epoch 48 finished. Val loss 0.08854296058416367, Val error 0.0341745531019979\n2020-08-15 11:51:44,131 Epoch 49 finished. Val loss 0.08885534852743149, Val error 0.035751840168243953\n2020-08-15 11:51:45,828 Epoch 50 finished. Val loss 0.10808652639389038, Val error 0.0473186119873817\n2020-08-15 11:51:47,559 Epoch 51 finished. Val loss 0.0957946702837944, Val error 0.03838065194532071\n2020-08-15 11:51:49,263 Epoch 52 finished. Val loss 0.09171731770038605, Val error 0.0341745531019979\n2020-08-15 11:51:50,962 Epoch 53 finished. Val loss 0.08858560770750046, Val error 0.03680336487907466\n2020-08-15 11:51:52,671 Epoch 54 finished. Val loss 0.09458788484334946, Val error 0.03627760252365931\n2020-08-15 11:51:54,366 Epoch 55 finished. Val loss 0.089992456138134, Val error 0.03470031545741325\n2020-08-15 11:51:56,133 Epoch 56 finished. Val loss 0.0941917672753334, Val error 0.03680336487907466\n2020-08-15 11:51:57,876 Epoch 57 finished. Val loss 0.09200669825077057, Val error 0.03838065194532071\n2020-08-15 11:51:59,742 Epoch 58 finished. Val loss 0.09282401204109192, Val error 0.035751840168243953\n2020-08-15 11:52:02,072 Epoch 59 finished. Val loss 0.09882906824350357, Val error 0.03627760252365931\n2020-08-15 11:52:04,597 Epoch 60 finished. Val loss 0.09534413367509842, Val error 0.03838065194532071\n2020-08-15 11:52:07,024 Epoch 61 finished. Val loss 0.08949919790029526, Val error 0.03470031545741325\n2020-08-15 11:52:09,705 Epoch 62 finished. Val loss 0.09106707572937012, Val error 0.03943217665615142\n2020-08-15 11:52:12,316 Epoch 63 finished. Val loss 0.09440477937459946, Val error 0.035226077812828605\n2020-08-15 11:52:14,620 Epoch 64 finished. Val loss 0.08962143957614899, Val error 0.033648790746582544\n2020-08-15 11:52:16,768 Epoch 65 finished. Val loss 0.12114101648330688, Val error 0.048370136698212406\n2020-08-15 11:52:18,893 Epoch 66 finished. Val loss 0.08843749016523361, Val error 0.035751840168243953\n2020-08-15 11:52:20,739 Epoch 67 finished. Val loss 0.08761467039585114, Val error 0.03470031545741325\n2020-08-15 11:52:22,599 Epoch 68 finished. Val loss 0.09171571582555771, Val error 0.03627760252365931\n2020-08-15 11:52:24,402 Epoch 69 finished. Val loss 0.10344958305358887, Val error 0.03943217665615142\n2020-08-15 11:52:26,411 Epoch 70 finished. Val loss 0.09664612263441086, Val error 0.03785488958990536\n2020-08-15 11:52:28,190 Epoch 71 finished. Val loss 0.09287845343351364, Val error 0.03838065194532071\n2020-08-15 11:52:30,043 Epoch 72 finished. Val loss 0.09591816365718842, Val error 0.035751840168243953\n2020-08-15 11:52:31,765 Epoch 73 finished. Val loss 0.08884380757808685, Val error 0.0341745531019979\n2020-08-15 11:52:33,572 Epoch 74 finished. Val loss 0.08996573835611343, Val error 0.035226077812828605\n2020-08-15 11:52:35,322 Epoch 75 finished. Val loss 0.09323444217443466, Val error 0.03470031545741325\n2020-08-15 11:52:37,382 Epoch 76 finished. Val loss 0.0909380093216896, Val error 0.0341745531019979\n2020-08-15 11:52:39,151 Epoch 77 finished. Val loss 0.09238233417272568, Val error 0.03785488958990536\n2020-08-15 11:52:40,916 Epoch 78 finished. Val loss 0.08862429112195969, Val error 0.035751840168243953\n2020-08-15 11:52:43,386 Epoch 79 finished. Val loss 0.08890912681818008, Val error 0.035226077812828605\n2020-08-15 11:52:46,765 Epoch 80 finished. Val loss 0.089668408036232, Val error 0.03470031545741325\n2020-08-15 11:52:49,302 Epoch 81 finished. Val loss 0.09077811241149902, Val error 0.033648790746582544\n2020-08-15 11:52:51,031 Epoch 82 finished. Val loss 0.0931752547621727, Val error 0.033123028391167195\n2020-08-15 11:52:52,762 Epoch 83 finished. Val loss 0.09031611680984497, Val error 0.03680336487907466\n2020-08-15 11:52:54,495 Epoch 84 finished. Val loss 0.09108369052410126, Val error 0.03785488958990536\n2020-08-15 11:52:56,281 Epoch 85 finished. Val loss 0.09739413857460022, Val error 0.03732912723449001\n2020-08-15 11:52:58,020 Epoch 86 finished. Val loss 0.09101646393537521, Val error 0.03732912723449001\n2020-08-15 11:52:59,796 Epoch 87 finished. Val loss 0.0897604301571846, Val error 0.03259726603575184\n2020-08-15 11:53:01,580 Epoch 88 finished. Val loss 0.10025999695062637, Val error 0.04153522607781283\n2020-08-15 11:53:04,477 Epoch 89 finished. Val loss 0.08998136222362518, Val error 0.035751840168243953\n2020-08-15 11:53:06,205 Epoch 90 finished. Val loss 0.09028994292020798, Val error 0.03627760252365931\n2020-08-15 11:53:07,915 Epoch 91 finished. Val loss 0.08908259123563766, Val error 0.035751840168243953\n2020-08-15 11:53:09,632 Epoch 92 finished. Val loss 0.09297184646129608, Val error 0.03470031545741325\n2020-08-15 11:53:11,569 Epoch 93 finished. Val loss 0.09832336753606796, Val error 0.04048370136698212\n2020-08-15 11:53:14,374 Epoch 94 finished. Val loss 0.08792481571435928, Val error 0.035226077812828605\n2020-08-15 11:53:16,146 Epoch 95 finished. Val loss 0.090321846306324, Val error 0.03259726603575184\n2020-08-15 11:53:17,938 Epoch 96 finished. Val loss 0.08870086818933487, Val error 0.035226077812828605\n2020-08-15 11:53:19,710 Epoch 97 finished. Val loss 0.09484992921352386, Val error 0.035226077812828605\n2020-08-15 11:53:21,483 Epoch 98 finished. Val loss 0.12815874814987183, Val error 0.04679284963196635\n2020-08-15 11:53:23,256 Epoch 99 finished. Val loss 0.09836730360984802, Val error 0.03680336487907466\n2020-08-15 11:53:25,048 Epoch 100 finished. Val loss 0.08905179798603058, Val error 0.035751840168243953\n2020-08-15 11:53:26,822 Epoch 101 finished. Val loss 0.09293849021196365, Val error 0.035226077812828605\n2020-08-15 11:53:28,589 Epoch 102 finished. Val loss 0.08969289809465408, Val error 0.035751840168243953\n2020-08-15 11:53:30,439 Epoch 103 finished. Val loss 0.08986487984657288, Val error 0.03470031545741325\n2020-08-15 11:53:32,172 Epoch 104 finished. Val loss 0.08979377895593643, Val error 0.03732912723449001\n2020-08-15 11:53:33,939 Epoch 105 finished. Val loss 0.09184660017490387, Val error 0.035226077812828605\n2020-08-15 11:53:35,688 Epoch 106 finished. Val loss 0.08991196751594543, Val error 0.03627760252365931\n2020-08-15 11:53:37,467 Epoch 107 finished. Val loss 0.08965067565441132, Val error 0.03470031545741325\n2020-08-15 11:53:39,228 Epoch 108 finished. Val loss 0.08977369219064713, Val error 0.03470031545741325\n2020-08-15 11:53:41,001 Epoch 109 finished. Val loss 0.0908472090959549, Val error 0.035226077812828605\n2020-08-15 11:53:42,792 Epoch 110 finished. Val loss 0.09486590325832367, Val error 0.03943217665615142\n2020-08-15 11:53:44,567 Epoch 111 finished. Val loss 0.09034714102745056, Val error 0.035226077812828605\n2020-08-15 11:53:46,328 Epoch 112 finished. Val loss 0.09614499658346176, Val error 0.03995793901156677\n2020-08-15 11:53:48,122 Epoch 113 finished. Val loss 0.08855351060628891, Val error 0.033123028391167195\n2020-08-15 11:53:49,896 Epoch 114 finished. Val loss 0.09343788772821426, Val error 0.033648790746582544\n2020-08-15 11:53:51,654 Epoch 115 finished. Val loss 0.09710735827684402, Val error 0.04048370136698212\n2020-08-15 11:53:53,470 Epoch 116 finished. Val loss 0.09632524847984314, Val error 0.03732912723449001\n2020-08-15 11:53:55,232 Epoch 117 finished. Val loss 0.09789180010557175, Val error 0.03890641430073607\n2020-08-15 11:53:57,010 Epoch 118 finished. Val loss 0.09460028260946274, Val error 0.03627760252365931\n2020-08-15 11:53:58,825 Epoch 119 finished. Val loss 0.08635009825229645, Val error 0.033648790746582544\n2020-08-15 11:54:00,790 Epoch 120 finished. Val loss 0.08869253844022751, Val error 0.0341745531019979\n2020-08-15 11:54:02,560 Epoch 121 finished. Val loss 0.0926317349076271, Val error 0.035751840168243953\n2020-08-15 11:54:04,350 Epoch 122 finished. Val loss 0.0918462872505188, Val error 0.035226077812828605\n2020-08-15 11:54:06,090 Epoch 123 finished. Val loss 0.09443939477205276, Val error 0.04153522607781283\n2020-08-15 11:54:07,853 Epoch 124 finished. Val loss 0.09872544556856155, Val error 0.04153522607781283\n2020-08-15 11:54:09,637 Epoch 125 finished. Val loss 0.0918055847287178, Val error 0.035226077812828605\n2020-08-15 11:54:11,420 Epoch 126 finished. Val loss 0.09513643383979797, Val error 0.035226077812828605\n2020-08-15 11:54:13,175 Epoch 127 finished. Val loss 0.09097837656736374, Val error 0.03732912723449001\n2020-08-15 11:54:14,957 Epoch 128 finished. Val loss 0.10695590078830719, Val error 0.04311251314405889\n2020-08-15 11:54:16,712 Epoch 129 finished. Val loss 0.08970768004655838, Val error 0.035226077812828605\n2020-08-15 11:54:18,535 Epoch 130 finished. Val loss 0.08981836587190628, Val error 0.033123028391167195\n2020-08-15 11:54:20,297 Epoch 131 finished. Val loss 0.09149667620658875, Val error 0.03680336487907466\n2020-08-15 11:54:22,593 Epoch 132 finished. Val loss 0.09518777579069138, Val error 0.03890641430073607\n2020-08-15 11:54:25,009 Epoch 133 finished. Val loss 0.09213531017303467, Val error 0.033648790746582544\n2020-08-15 11:54:26,790 Epoch 134 finished. Val loss 0.08717814832925797, Val error 0.030494216614090432\n2020-08-15 11:54:28,558 Epoch 135 finished. Val loss 0.09855882823467255, Val error 0.03785488958990536\n2020-08-15 11:54:30,349 Epoch 136 finished. Val loss 0.08771973103284836, Val error 0.033648790746582544\n2020-08-15 11:54:32,091 Epoch 137 finished. Val loss 0.09196276217699051, Val error 0.035751840168243953\n2020-08-15 11:54:33,837 Epoch 138 finished. Val loss 0.09247255325317383, Val error 0.03627760252365931\n2020-08-15 11:54:35,590 Epoch 139 finished. Val loss 0.09796992689371109, Val error 0.04048370136698212\n2020-08-15 11:54:37,343 Epoch 140 finished. Val loss 0.08705628663301468, Val error 0.03259726603575184\n2020-08-15 11:54:39,100 Epoch 141 finished. Val loss 0.10546941310167313, Val error 0.04100946372239748\n2020-08-15 11:54:40,853 Epoch 142 finished. Val loss 0.09011756628751755, Val error 0.03680336487907466\n2020-08-15 11:54:42,635 Epoch 143 finished. Val loss 0.0888524055480957, Val error 0.03470031545741325\n2020-08-15 11:54:44,729 Epoch 144 finished. Val loss 0.09434874355792999, Val error 0.03890641430073607\n2020-08-15 11:54:46,463 Epoch 145 finished. Val loss 0.08954323828220367, Val error 0.03890641430073607\n2020-08-15 11:54:48,243 Epoch 146 finished. Val loss 0.09035930782556534, Val error 0.03470031545741325\n2020-08-15 11:54:49,982 Epoch 147 finished. Val loss 0.08768834918737411, Val error 0.03470031545741325\n2020-08-15 11:54:51,727 Epoch 148 finished. Val loss 0.08971528708934784, Val error 0.035751840168243953\n2020-08-15 11:54:54,129 Epoch 149 finished. Val loss 0.09233465045690536, Val error 0.03732912723449001\n2020-08-15 11:54:55,869 Epoch 150 finished. Val loss 0.08790486305952072, Val error 0.03259726603575184\n2020-08-15 11:54:57,647 Epoch 151 finished. Val loss 0.09559158980846405, Val error 0.035226077812828605\n2020-08-15 11:54:59,455 Epoch 152 finished. Val loss 0.11138997972011566, Val error 0.04258675078864353\n2020-08-15 11:55:01,293 Epoch 153 finished. Val loss 0.09287742525339127, Val error 0.03890641430073607\n2020-08-15 11:55:03,004 Epoch 154 finished. Val loss 0.09580180048942566, Val error 0.035751840168243953\n2020-08-15 11:55:04,765 Epoch 155 finished. Val loss 0.09363789111375809, Val error 0.035751840168243953\n2020-08-15 11:55:06,531 Epoch 156 finished. Val loss 0.08976398408412933, Val error 0.035226077812828605\n2020-08-15 11:55:08,315 Epoch 157 finished. Val loss 0.09376857429742813, Val error 0.03627760252365931\n2020-08-15 11:55:10,064 Epoch 158 finished. Val loss 0.09140124917030334, Val error 0.035751840168243953\n2020-08-15 11:55:11,823 Epoch 159 finished. Val loss 0.09173472970724106, Val error 0.03470031545741325\n2020-08-15 11:55:13,695 Epoch 160 finished. Val loss 0.0893150195479393, Val error 0.03627760252365931\n2020-08-15 11:55:16,375 Epoch 161 finished. Val loss 0.09015316516160965, Val error 0.03785488958990536\n2020-08-15 11:55:18,578 Epoch 162 finished. Val loss 0.09897106140851974, Val error 0.04100946372239748\n2020-08-15 11:55:20,820 Epoch 163 finished. Val loss 0.08944546431303024, Val error 0.03627760252365931\n2020-08-15 11:55:22,775 Epoch 164 finished. Val loss 0.09394612908363342, Val error 0.03890641430073607\n2020-08-15 11:55:24,630 Epoch 165 finished. Val loss 0.10800501704216003, Val error 0.0452155625657203\n2020-08-15 11:55:26,512 Epoch 166 finished. Val loss 0.09240730851888657, Val error 0.035226077812828605\n2020-08-15 11:55:28,425 Epoch 167 finished. Val loss 0.08961508423089981, Val error 0.0341745531019979\n2020-08-15 11:55:30,488 Epoch 168 finished. Val loss 0.09061191976070404, Val error 0.03732912723449001\n2020-08-15 11:55:32,362 Epoch 169 finished. Val loss 0.09676718711853027, Val error 0.04048370136698212\n2020-08-15 11:55:34,217 Epoch 170 finished. Val loss 0.0936848521232605, Val error 0.03627760252365931\n2020-08-15 11:55:36,090 Epoch 171 finished. Val loss 0.09053162485361099, Val error 0.03627760252365931\n2020-08-15 11:55:37,970 Epoch 172 finished. Val loss 0.09617342799901962, Val error 0.03838065194532071\n2020-08-15 11:55:40,199 Epoch 173 finished. Val loss 0.09788019210100174, Val error 0.03995793901156677\n2020-08-15 11:55:43,354 Epoch 174 finished. Val loss 0.08924989402294159, Val error 0.035751840168243953\n2020-08-15 11:55:45,106 Epoch 175 finished. Val loss 0.0892861932516098, Val error 0.035751840168243953\n2020-08-15 11:55:48,003 Epoch 176 finished. Val loss 0.09248050302267075, Val error 0.03732912723449001\n2020-08-15 11:55:50,447 Epoch 177 finished. Val loss 0.0943346619606018, Val error 0.035751840168243953\n2020-08-15 11:55:52,703 Epoch 178 finished. Val loss 0.08978866040706635, Val error 0.03732912723449001\n2020-08-15 11:55:55,960 Epoch 179 finished. Val loss 0.09757521003484726, Val error 0.03995793901156677\n2020-08-15 11:55:58,619 Epoch 180 finished. Val loss 0.0912010446190834, Val error 0.033648790746582544\n2020-08-15 11:56:01,097 Epoch 181 finished. Val loss 0.08988261967897415, Val error 0.03627760252365931\n2020-08-15 11:56:03,773 Epoch 182 finished. Val loss 0.09037015587091446, Val error 0.035751840168243953\n2020-08-15 11:56:06,730 Epoch 183 finished. Val loss 0.09175697714090347, Val error 0.03732912723449001\n2020-08-15 11:56:09,272 Epoch 184 finished. Val loss 0.10431621223688126, Val error 0.04258675078864353\n2020-08-15 11:56:11,774 Epoch 185 finished. Val loss 0.09501323848962784, Val error 0.03732912723449001\n2020-08-15 11:56:14,361 Epoch 186 finished. Val loss 0.09640488773584366, Val error 0.03943217665615142\n2020-08-15 11:56:16,720 Epoch 187 finished. Val loss 0.0927838459610939, Val error 0.03627760252365931\n2020-08-15 11:56:19,070 Epoch 188 finished. Val loss 0.11895791441202164, Val error 0.050473186119873815\n2020-08-15 11:56:21,316 Epoch 189 finished. Val loss 0.08664922416210175, Val error 0.03259726603575184\n2020-08-15 11:56:23,809 Epoch 190 finished. Val loss 0.09895577281713486, Val error 0.035226077812828605\n2020-08-15 11:56:25,598 Epoch 191 finished. Val loss 0.09092976897954941, Val error 0.03680336487907466\n2020-08-15 11:56:27,338 Epoch 192 finished. Val loss 0.08854129910469055, Val error 0.03470031545741325\n2020-08-15 11:56:29,106 Epoch 193 finished. Val loss 0.0912376344203949, Val error 0.033648790746582544\n2020-08-15 11:56:31,054 Epoch 194 finished. Val loss 0.08923573791980743, Val error 0.035751840168243953\n2020-08-15 11:56:32,910 Epoch 195 finished. Val loss 0.09374988079071045, Val error 0.03732912723449001\n2020-08-15 11:56:34,729 Epoch 196 finished. Val loss 0.09234806150197983, Val error 0.03890641430073607\n2020-08-15 11:56:36,822 Epoch 197 finished. Val loss 0.09001803398132324, Val error 0.035751840168243953\n2020-08-15 11:56:38,974 Epoch 198 finished. Val loss 0.09122436493635178, Val error 0.03680336487907466\n2020-08-15 11:56:41,122 Epoch 199 finished. Val loss 0.09328705817461014, Val error 0.04100946372239748\n"
    }
   ],
   "source": [
    "trainer.train(n_epoch=n_epoch, burn_in_epochs=burn_in_epochs, resample_prior_until=resample_prior_until)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраняем сэмплы весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "25\n"
    }
   ],
   "source": [
    "weights_set = trainer.weight_set_samples[-(n_epoch - resample_prior_until) // save_freq:]\n",
    "\n",
    "print(len(weights_set))\n",
    "\n",
    "Path('../saved_samples', 'mnist_weights').mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(weights_set, Path('../saved_samples', 'mnist_weights', 'weights.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_set = pickle.load(Path('../saved_samples', 'mnist_weights', 'weights.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_set = weights_set[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "25"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(weights_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from control_variates.cv_utils import state_dict_to_vec\n",
    "from control_variates.cv_utils import compute_log_likelihood, compute_mc_estimate, compute_naive_variance, compute_tricky_divergence\n",
    "from control_variates.model import get_prediction, get_binary_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_weights = [state_dict_to_vec(w) for w in weights_set]\n",
    "\n",
    "models = [LogRegression(input_dim) for _ in range(len(weights_set))]\n",
    "for w, model in zip(weights_set, models):\n",
    "    model.load_state_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'weight_decay'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7d737b47fe60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_with_priors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpriors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../saved_samples'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mnist_weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'priors.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'weight_decay'"
     ]
    }
   ],
   "source": [
    "opt_with_priors = trainer.optimizer\n",
    "\n",
    "priors = {}\n",
    "group_params = opt_with_priors.param_groups[0]['params']\n",
    "for (n, _), p in zip(model.named_parameters(), group_params):  \n",
    "    state = opt_with_priors.state[p]  \n",
    "    priors[n] = state['weight_decay']\n",
    "\n",
    "pickle.dump(priors, Path('../saved_samples', 'mnist_weights', 'priors.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = pickle.load(Path('../saved_samples', 'mnist_weights', 'priors.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from control_variates.cv import PsyMLP, PsyDoubleMLP, PsyLinear, SteinCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_hidden = 150\n",
    "psy_depth1 = 3\n",
    "psy_depth2 = 2\n",
    "n_iter = 1000\n",
    "psy_lr = 1e-2\n",
    "psy_input1 = squeezed_weights[0].shape[0]\n",
    "N_train = len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, new_valloader = load_mnist_dataset(Path('../data', 'mnist'), batch_size, [4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = next(iter(new_valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = next(iter(valloader))\n",
    "#x_new = x_new\n",
    "train_x, train_y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фитим на батче"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#psy_model = PsyMLP(psy_input1, psy_hidden, psy_depth1)\n",
    "psy_model = PsyLinear(psy_input1)\n",
    "psy_model.init_zero()\n",
    "psy_model.to(device)\n",
    "\n",
    "neural_control_variate = SteinCV(psy_model, train_x, train_y, priors, N_train)\n",
    "\n",
    "ncv_optimizer = torch.optim.Adam(psy_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "25"
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16])tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16])tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.])tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16])tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([1.3323e-14]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-15]) tensor([0.0012])\ntensor([5.3291e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([9.7700e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.6875e-14]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-14]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.4211e-14]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([9.7700e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([5.3291e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([7.9936e-15])tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.1546e-14]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([9.7700e-15]) tensor([0.0012])\ntensor([1.2434e-14]) tensor([0.0012])\ntensor([1.2434e-14]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15])tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([7.1054e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([1.7764e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([4.4409e-15]) tensor([0.0012])\ntensor([3.5527e-15]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\ntensor([6.2172e-15]) tensor([0.0012])\ntensor([0.]) tensor([0.0012])\ntensor([8.8818e-16]) tensor([0.0012])\ntensor([2.6645e-15]) tensor([0.0012])\n"
    }
   ],
   "source": [
    "function_f = lambda model, x: get_binary_prediction(model, x, classes=[0, 1])\n",
    "\n",
    "for it in range(n_iter):\n",
    "    for x in x_new[30:31]:\n",
    "        ncv_optimizer.zero_grad()\n",
    "        mc_variance, no_cv_variance = compute_naive_variance(function_f, neural_control_variate, models[::5], x)\n",
    "        print(mc_variance.data, no_cv_variance.data)\n",
    "        mc_variance.backward()\n",
    "        # for n, p in psy_model.named_parameters():\n",
    "        #     print(n, p, p.grad)\n",
    "        ncv_optimizer.step()\n",
    "        # for n, p in psy_model.named_parameters():\n",
    "        #     print(n, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([('layer.weight',\n              tensor([[-0.0445,  0.0381,  0.0149,  ...,  0.0248,  0.0312, -0.0313]]))])"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "psy_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([-0.0049, -0.0049, -0.0049,  ...,  0.0049,  0.0049, -0.0049]),)"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "torch.autograd.grad(psy_model.forward(squeezed_weights[0], None), squeezed_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([-0.0049, -0.0049, -0.0049,  ...,  0.0049,  0.0049, -0.0049]),)"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "torch.autograd.grad(psy_model.forward(squeezed_weights[0], None), squeezed_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}