{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import MLP\n",
    "from optimizers import SGLD, H_SA_SGHMC\n",
    "from mnist_utils import load_mnist_dataset\n",
    "from trainer import BNNTrainer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "input_dim = 784\n",
    "width = 100\n",
    "depth = 2\n",
    "output_dim = 10\n",
    "lr = 1e-3\n",
    "n_epoch = 100\n",
    "alpha0, beta0 = 10, 10\n",
    "resample_prior_every = 15\n",
    "resample_momentum_every = 50\n",
    "burn_in_epochs = 10\n",
    "save_freq = 2\n",
    "resample_prior_until = 50\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, valloader = load_mnist_dataset('data', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGLD(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "optimizer = H_SA_SGHMC(model.parameters(), lr=lr, alpha0=alpha0, beta0=beta0)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_func(y_hat, y):\n",
    "    nll = F.cross_entropy(y_hat, y, reduction='sum')\n",
    "    return nll\n",
    "\n",
    "def err_func(y_hat, y):\n",
    "    err = y_hat.argmax(-1).ne(y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BNNTrainer(model, optimizer, nll_func, err_func, trainloader, valloader, device=device, \n",
    "    resample_prior_every=resample_prior_every,\n",
    "    resample_momentum_every=resample_momentum_every,\n",
    "    save_freq=save_freq,\n",
    "    batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-08-14 10:33:00,364 Epoch 0 finished. Val loss 0.16795885562896729, Val error 0.0522\n2020-08-14 10:33:13,539 Epoch 1 finished. Val loss 0.14774039387702942, Val error 0.0449\n2020-08-14 10:33:23,898 Epoch 2 finished. Val loss 0.15454192459583282, Val error 0.0468\n2020-08-14 10:33:34,645 Epoch 3 finished. Val loss 0.13956117630004883, Val error 0.0395\n2020-08-14 10:33:43,440 Epoch 4 finished. Val loss 0.14896883070468903, Val error 0.0391\n2020-08-14 10:33:51,539 Epoch 5 finished. Val loss 0.1541183441877365, Val error 0.0404\n2020-08-14 10:34:00,327 Epoch 6 finished. Val loss 0.14277158677577972, Val error 0.0378\n2020-08-14 10:34:07,655 Epoch 7 finished. Val loss 0.16034924983978271, Val error 0.0391\n2020-08-14 10:34:14,958 Epoch 8 finished. Val loss 0.19193580746650696, Val error 0.0474\n2020-08-14 10:34:22,457 Epoch 9 finished. Val loss 0.1724758744239807, Val error 0.0404\n2020-08-14 10:34:30,700 Epoch 10 finished. Val loss 0.1634906530380249, Val error 0.0395\n2020-08-14 10:34:39,268 Epoch 11 finished. Val loss 0.19305537641048431, Val error 0.0443\n2020-08-14 10:34:46,671 Epoch 12 finished. Val loss 0.1769949495792389, Val error 0.0406\n2020-08-14 10:34:54,060 Epoch 13 finished. Val loss 0.19188480079174042, Val error 0.041\n2020-08-14 10:35:01,779 Epoch 14 finished. Val loss 0.18876859545707703, Val error 0.0406\n2020-08-14 10:35:11,244 Epoch 15 finished. Val loss 0.21103879809379578, Val error 0.0398\n2020-08-14 10:35:18,538 Epoch 16 finished. Val loss 0.2147546112537384, Val error 0.0419\n2020-08-14 10:35:27,343 Epoch 17 finished. Val loss 0.20807763934135437, Val error 0.0375\n2020-08-14 10:35:34,654 Epoch 18 finished. Val loss 0.23236806690692902, Val error 0.0442\n2020-08-14 10:35:42,587 Epoch 19 finished. Val loss 0.2312215119600296, Val error 0.0415\n2020-08-14 10:35:50,900 Epoch 20 finished. Val loss 0.21854348480701447, Val error 0.0404\n2020-08-14 10:35:58,259 Epoch 21 finished. Val loss 0.24909375607967377, Val error 0.0419\n2020-08-14 10:36:06,678 Epoch 22 finished. Val loss 0.24367034435272217, Val error 0.0396\n2020-08-14 10:36:22,137 Epoch 23 finished. Val loss 0.2507951855659485, Val error 0.0398\n2020-08-14 10:36:36,041 Epoch 24 finished. Val loss 0.22752808034420013, Val error 0.0384\n2020-08-14 10:36:46,240 Epoch 25 finished. Val loss 0.2934088706970215, Val error 0.0397\n2020-08-14 10:36:54,845 Epoch 26 finished. Val loss 0.2594243884086609, Val error 0.0374\n2020-08-14 10:37:03,299 Epoch 27 finished. Val loss 0.26977410912513733, Val error 0.0403\n2020-08-14 10:37:11,717 Epoch 28 finished. Val loss 0.2889814078807831, Val error 0.0377\n2020-08-14 10:37:20,271 Epoch 29 finished. Val loss 0.3028748333454132, Val error 0.0407\n2020-08-14 10:37:28,523 Epoch 30 finished. Val loss 0.25361695885658264, Val error 0.0386\n2020-08-14 10:37:36,784 Epoch 31 finished. Val loss 0.30744433403015137, Val error 0.0398\n2020-08-14 10:37:45,140 Epoch 32 finished. Val loss 0.2879159450531006, Val error 0.0369\n2020-08-14 10:37:53,468 Epoch 33 finished. Val loss 0.29912301898002625, Val error 0.0387\n2020-08-14 10:38:01,770 Epoch 34 finished. Val loss 0.2911694049835205, Val error 0.0373\n2020-08-14 10:38:10,376 Epoch 35 finished. Val loss 0.27331244945526123, Val error 0.0365\n2020-08-14 10:38:20,915 Epoch 36 finished. Val loss 0.2763582170009613, Val error 0.0351\n2020-08-14 10:38:30,455 Epoch 37 finished. Val loss 0.26488053798675537, Val error 0.0328\n2020-08-14 10:38:38,882 Epoch 38 finished. Val loss 0.2899506688117981, Val error 0.0382\n2020-08-14 10:38:47,375 Epoch 39 finished. Val loss 0.3049045205116272, Val error 0.0414\n2020-08-14 10:38:56,119 Epoch 40 finished. Val loss 0.3216916620731354, Val error 0.0391\n2020-08-14 10:39:09,061 Epoch 41 finished. Val loss 0.29727599024772644, Val error 0.036\n2020-08-14 10:39:19,748 Epoch 42 finished. Val loss 0.3095458149909973, Val error 0.0383\n2020-08-14 10:39:35,553 Epoch 43 finished. Val loss 0.3211727440357208, Val error 0.0395\n2020-08-14 10:39:43,600 Epoch 44 finished. Val loss 0.32888972759246826, Val error 0.0412\n2020-08-14 10:39:53,369 Epoch 45 finished. Val loss 0.3644714653491974, Val error 0.04\n2020-08-14 10:40:01,598 Epoch 46 finished. Val loss 0.34360530972480774, Val error 0.0375\n2020-08-14 10:40:09,466 Epoch 47 finished. Val loss 0.37729910016059875, Val error 0.0414\n2020-08-14 10:40:17,385 Epoch 48 finished. Val loss 0.3775271475315094, Val error 0.0397\n2020-08-14 10:40:26,272 Epoch 49 finished. Val loss 0.35776010155677795, Val error 0.0401\n2020-08-14 10:40:35,094 Epoch 50 finished. Val loss 0.37128958106040955, Val error 0.0376\n2020-08-14 10:40:43,006 Epoch 51 finished. Val loss 0.38806235790252686, Val error 0.0374\n2020-08-14 10:40:50,959 Epoch 52 finished. Val loss 0.37552255392074585, Val error 0.0393\n2020-08-14 10:40:58,892 Epoch 53 finished. Val loss 0.37386995553970337, Val error 0.0391\n2020-08-14 10:41:06,786 Epoch 54 finished. Val loss 0.40465208888053894, Val error 0.0385\n2020-08-14 10:41:16,466 Epoch 55 finished. Val loss 0.36595526337623596, Val error 0.038\n2020-08-14 10:41:24,865 Epoch 56 finished. Val loss 0.3464595079421997, Val error 0.0357\n2020-08-14 10:41:33,708 Epoch 57 finished. Val loss 0.3745870590209961, Val error 0.038\n2020-08-14 10:41:44,894 Epoch 58 finished. Val loss 0.36178749799728394, Val error 0.0372\n2020-08-14 10:41:53,213 Epoch 59 finished. Val loss 0.4447998106479645, Val error 0.0366\n2020-08-14 10:42:01,069 Epoch 60 finished. Val loss 0.3753812909126282, Val error 0.0371\n2020-08-14 10:42:10,639 Epoch 61 finished. Val loss 0.3470478951931, Val error 0.0369\n2020-08-14 10:42:19,027 Epoch 62 finished. Val loss 0.37357157468795776, Val error 0.0369\n2020-08-14 10:42:27,356 Epoch 63 finished. Val loss 0.3700982928276062, Val error 0.0382\n2020-08-14 10:42:35,174 Epoch 64 finished. Val loss 0.3588559627532959, Val error 0.0372\n2020-08-14 10:42:44,914 Epoch 65 finished. Val loss 0.3786719739437103, Val error 0.034\n2020-08-14 10:42:55,089 Epoch 66 finished. Val loss 0.3806038498878479, Val error 0.0366\n2020-08-14 10:43:05,107 Epoch 67 finished. Val loss 0.37204310297966003, Val error 0.0346\n2020-08-14 10:43:14,632 Epoch 68 finished. Val loss 0.4090183675289154, Val error 0.0372\n2020-08-14 10:43:25,510 Epoch 69 finished. Val loss 0.4391942024230957, Val error 0.0376\n2020-08-14 10:43:35,361 Epoch 70 finished. Val loss 0.40932074189186096, Val error 0.0359\n2020-08-14 10:43:43,706 Epoch 71 finished. Val loss 0.38488754630088806, Val error 0.0358\n2020-08-14 10:43:52,862 Epoch 72 finished. Val loss 0.4355751872062683, Val error 0.0375\n2020-08-14 10:44:02,249 Epoch 73 finished. Val loss 0.48218783736228943, Val error 0.0387\n2020-08-14 10:44:12,806 Epoch 74 finished. Val loss 0.442783385515213, Val error 0.0389\n2020-08-14 10:44:22,152 Epoch 75 finished. Val loss 0.4099298417568207, Val error 0.0356\n2020-08-14 10:44:30,156 Epoch 76 finished. Val loss 0.4483461081981659, Val error 0.0355\n2020-08-14 10:44:38,035 Epoch 77 finished. Val loss 0.47882452607154846, Val error 0.0376\n2020-08-14 10:44:47,585 Epoch 78 finished. Val loss 0.4931887686252594, Val error 0.0384\n2020-08-14 10:44:57,051 Epoch 79 finished. Val loss 0.4967494606971741, Val error 0.0371\n2020-08-14 10:45:05,977 Epoch 80 finished. Val loss 0.5019606351852417, Val error 0.0356\n2020-08-14 10:45:15,901 Epoch 81 finished. Val loss 0.5553150773048401, Val error 0.0398\n2020-08-14 10:45:24,299 Epoch 82 finished. Val loss 0.5892508029937744, Val error 0.0405\n2020-08-14 10:45:33,849 Epoch 83 finished. Val loss 0.5086578726768494, Val error 0.0361\n2020-08-14 10:45:42,778 Epoch 84 finished. Val loss 0.5194754004478455, Val error 0.0366\n2020-08-14 10:45:52,290 Epoch 85 finished. Val loss 0.4089029133319855, Val error 0.0374\n2020-08-14 10:46:01,246 Epoch 86 finished. Val loss 0.4834006428718567, Val error 0.0363\n2020-08-14 10:46:09,971 Epoch 87 finished. Val loss 0.49480581283569336, Val error 0.0364\n2020-08-14 10:46:20,157 Epoch 88 finished. Val loss 0.4860488176345825, Val error 0.0352\n2020-08-14 10:46:29,526 Epoch 89 finished. Val loss 0.5436832308769226, Val error 0.0348\n2020-08-14 10:46:40,357 Epoch 90 finished. Val loss 0.45072147250175476, Val error 0.0344\n2020-08-14 10:46:49,519 Epoch 91 finished. Val loss 0.5486648678779602, Val error 0.0382\n2020-08-14 10:46:57,900 Epoch 92 finished. Val loss 0.5558451414108276, Val error 0.0398\n2020-08-14 10:47:08,076 Epoch 93 finished. Val loss 0.5629831552505493, Val error 0.0389\n2020-08-14 10:47:18,905 Epoch 94 finished. Val loss 0.5286007523536682, Val error 0.0383\n2020-08-14 10:47:27,473 Epoch 95 finished. Val loss 0.5699313282966614, Val error 0.0382\n2020-08-14 10:47:36,540 Epoch 96 finished. Val loss 0.4791906177997589, Val error 0.0392\n2020-08-14 10:47:47,848 Epoch 97 finished. Val loss 0.5641924738883972, Val error 0.038\n2020-08-14 10:47:57,580 Epoch 98 finished. Val loss 0.49355316162109375, Val error 0.0355\n2020-08-14 10:48:06,711 Epoch 99 finished. Val loss 0.5848889350891113, Val error 0.0355\n"
    }
   ],
   "source": [
    "trainer.train(n_epoch=n_epoch, burn_in_epochs=burn_in_epochs, resample_prior_until=resample_prior_until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_with_priors = trainer.optimizer\n",
    "\n",
    "priors = {}\n",
    "group_params = opt_with_priors.param_groups[0]['params']\n",
    "for (n, _), p in zip(model.named_parameters(), group_params):  \n",
    "    state = opt_with_priors.state[p]  \n",
    "    priors[n] = state['weight_decay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_set = trainer.weight_set_samples[-(n_epoch - resample_prior_until) // 2:]\n",
    "pickle.dump(weights_set, Path('weights.pkl').open('wb'))\n",
    "\n",
    "def state_dict_to_vec(state_dict):\n",
    "    return torch.cat([w_i.view(-1) for w_i in state_dict.values()])\n",
    "\n",
    "squeezed_weights = [state_dict_to_vec(w) for w in weights_set]\n",
    "\n",
    "models = [MLP(input_dim=input_dim, width=width, depth=depth, output_dim=output_dim) for w in weights_set]\n",
    "for w, model in zip(weights_set, models):\n",
    "    model.load_state_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([89610])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "squeezed_weights[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x, model):\n",
    "    return F.softmax(model(x), dim=-1)\n",
    "\n",
    "def get_binary_prediction(x, model, classes):\n",
    "    assert len(classes) == 2\n",
    "    return F.softmax(model(x)[..., classes], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(x, y, model):\n",
    "    assert len(classes) = 2\n",
    "    y_hat = model(x)\n",
    "    log_likelihood = -F.cross_entropy(y_hat, y, reduction='mean')\n",
    "\n",
    "    return log_likelihood\n",
    "\n",
    "def compute_mc_estimate(function: callable, models, x: torch.tensor):\n",
    "    res = 0.0\n",
    "    for model in models:\n",
    "        res += function(x, model)\n",
    "    return res / len(models)\n",
    "\n",
    "def compute_naive_variance(function:callable, control_variate: callable, models, x: torch.tensor):\n",
    "    return torch.sum(torch.tensor([(function(x, model) - control_variate(x, model))**2 for model in models])) / (len(models) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tricky_divergence(model):\n",
    "    div = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            div += p.grad.sum()\n",
    "\n",
    "    return div\n",
    "\n",
    "def stein_control_variate(psy_function, train_x, train_y, model, opt_with_priors, N_train, new_x):\n",
    "    log_likelihood = compute_log_likelihood(train_x, train_y, model) * N_train\n",
    "    log_likelihood.backward()\n",
    "    \n",
    "    ncv_value = 0\n",
    "    psy_value = psy_function(state_dict_to_vec(model.state_dict()), new_x)\n",
    "    psy_value.backward()\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        d_p = p.grad.data\n",
    "        d_p.add_(p, alpha=-priors[n])\n",
    "\n",
    "        psy_value += \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    weight = state_dict_to_vec(model.state_dict())\n",
    "    phi_weigth = phi_function(weight, x)\n",
    "    phi_weight.backward()\n",
    "    \n",
    "    control_variate = \n",
    "\n",
    "def wan_control_variate(psy_function, model, x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PsyLinear(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        return  self.layer(weights)\n",
    "\n",
    "class PsyMLP(nn.Module):\n",
    "    def __init__(self, input_dim, width, depth):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "        layers = [nn.Linear(input_dim, width), nn.ReLU()]\n",
    "        for i in range(depth - 1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(width, 1))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PsyLinear(\n  (layer): Linear(in_features=89610, out_features=1, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "psy_linear = PsyLinear(squeezed_weights[0].shape[0])\n",
    "\n",
    "psy_linear.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = psy_linear(squeezed_weights[0], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.1204], grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  }
 ]
}