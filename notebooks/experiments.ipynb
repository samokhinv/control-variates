{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загружаем сэмплы\n",
    "2. Новые точки\n",
    "3. Считаем для них MC оценки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "args = {\n",
    "    #'bnn_lr': 1e-3,\n",
    "    'cv_lr': 1e-7,\n",
    "    'n_cv_iter': 100,\n",
    "    'batch_size' : 20000,\n",
    "    'input_dim' : 784,\n",
    "    'width' : 100,\n",
    "    'depth' : 2,\n",
    "    'output_dim' : 2,\n",
    "    #'n_epoch' : 200,\n",
    "    #'alpha0' : 10, \n",
    "    #'beta0' : 10,\n",
    "    #'resample_prior_every' : 15,\n",
    "    #'resample_momentum_every' : 50,\n",
    "    #'burn_in_epochs' : 20,\n",
    "    #'save_freq' : 4,\n",
    "    #'resample_prior_until' : 100,\n",
    "    #'report_every' : 10,\n",
    "    }\n",
    "\n",
    "args = edict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "with open('../saved_samples/mnist_weights/30_samples_seed-1.pkl', 'rb') as fp:\n",
    "    samples = pickle.load(fp)\n",
    "\n",
    "#samples = samples[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_variates.model import LogRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_utils import load_mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = load_mnist_dataset('../data/mnist/', args.batch_size, classes=[3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = [(x[0][-10:], x[1]) for x in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = [[LogRegression(args.input_dim)\n",
    "                 for j in range(len(samples[i][0]))]\n",
    "                for i in range(len(samples))]\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    for j in range(len(samples[i][0])):\n",
    "        trajectories[i][j].load_state_dict(samples[i][0][j])\n",
    "\n",
    "priors = [samples[i][1] for i in range(len(samples))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogRegression(\n  (linear): Linear(in_features=784, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "trajectories[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = next(iter(valid_dl))\n",
    "train_x, train_y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фитим ncv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_variates.cv import PsyLinear, SteinCV, PsyConstVector, PsyMLP\n",
    "from control_variates.cv_utils import state_dict_to_vec, compute_naive_variance\n",
    "from control_variates.model import get_binary_prediction\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_variates.uncertainty_quantification import ClassificationUncertaintyMCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_input_dim = 1570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centr_regularizer(ncv, models, x, ll_div=None):\n",
    "    return (ncv(models, x, ll_div).mean(0))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_variates.cv_utils import compute_log_likelihood, compute_concat_gradient\n",
    "\n",
    "def compute_ll_div(models, train_x, train_y, N_train, priors=None):\n",
    "    for model in models:\n",
    "        model.zero_grad()\n",
    "    log_likelihoods = [(compute_log_likelihood(train_x, train_y, model) * N_train).backward() for model in models]\n",
    "    ll_div = torch.stack([compute_concat_gradient(model, priors) for model in models])\n",
    "    return ll_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "1%|          | 1/100 [00:01<01:41,  1.03s/it]hi\n  2%|▏         | 2/100 [00:02<01:40,  1.02s/it]hi\n  3%|▎         | 3/100 [00:03<01:41,  1.05s/it]hi\n  4%|▍         | 4/100 [00:04<01:37,  1.01s/it]hi\n  5%|▌         | 5/100 [00:05<01:38,  1.04s/it]hi\n  6%|▌         | 6/100 [00:06<01:47,  1.14s/it]hi\n  7%|▋         | 7/100 [00:07<01:42,  1.10s/it]hi\n  8%|▊         | 8/100 [00:08<01:42,  1.11s/it]hi\n  9%|▉         | 9/100 [00:09<01:38,  1.08s/it]hi\n 10%|█         | 10/100 [00:10<01:32,  1.03s/it]hi\n 11%|█         | 11/100 [00:11<01:29,  1.00s/it]hi\n 12%|█▏        | 12/100 [00:12<01:38,  1.12s/it]hi\n 13%|█▎        | 13/100 [00:13<01:32,  1.06s/it]hi\n 14%|█▍        | 14/100 [00:14<01:24,  1.01it/s]hi\n 15%|█▌        | 15/100 [00:15<01:18,  1.08it/s]hi\n 16%|█▌        | 16/100 [00:16<01:24,  1.01s/it]hi\n 17%|█▋        | 17/100 [00:18<01:32,  1.12s/it]hi\n 18%|█▊        | 18/100 [00:19<01:30,  1.10s/it]hi\n 19%|█▉        | 19/100 [00:20<01:40,  1.24s/it]hi\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ec6a95d2ad79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmc_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_cv_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_naive_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_control_variate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_variance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mmc_variance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcentr_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_control_variate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mncv_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mncv_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_control_variate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-0ccfec090cf3>\u001b[0m in \u001b[0;36mcentr_regularizer\u001b[0;34m(ncv, models, x, ll_div)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcentr_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mncv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/control-variates/control_variates/cv.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, models, x_batch, ll_div)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mll_div\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mlog_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;31m#ll_div = torch.stack([compute_tricky_divergence(model, self.priors) for model in models])  # ll_div для каждой модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mll_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_concat_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/control-variates/control_variates/cv.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mll_div\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mlog_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;31m#ll_div = torch.stack([compute_tricky_divergence(model, self.priors) for model in models])  # ll_div для каждой модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mll_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompute_concat_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/control-variates/control_variates/cv_utils.py\u001b[0m in \u001b[0;36mcompute_log_likelihood\u001b[0;34m(x, y, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ncv_s = []\n",
    "squeezed_weights = [[state_dict_to_vec(samples[i][0][j]) \n",
    "                     for j in range(len(samples[i][0]))]\n",
    "                    for i in range(len(samples))]\n",
    "psy_input_dim = squeezed_weights[0][0].shape[0]\n",
    "\n",
    "for models, pr in zip(trajectories, priors):\n",
    "    #psy_model = PsyConv(psy_input_dim, 75)\n",
    "    psy_model = PsyConstVector(psy_input_dim)\n",
    "    #psy_model = PsyLinear(psy_input_dim)\n",
    "    #psy_model = PsyMLP(psy_input_dim, 50, 1)\n",
    "    psy_model.init_zero()\n",
    "    psy_model.to(device)\n",
    "\n",
    "    neural_control_variate = SteinCV(psy_model, train_x, train_y, pr, N_train)\n",
    "\n",
    "    ncv_optimizer = torch.optim.Adam(psy_model.parameters(), lr=args.cv_lr, weight_decay=0.0) #1e-4)\n",
    "    uncertainty_quant = ClassificationUncertaintyMCMC(models, neural_control_variate)\n",
    "\n",
    "    train_x, train_y = next(iter(train_dl))\n",
    "    ll_div = compute_ll_div(models, train_x, train_y, N_train, priors=priors)\n",
    "\n",
    "    function_f = lambda model, x: get_binary_prediction(model, x, classes=[0, 1])\n",
    "    history = [] \n",
    "    x = x_new[13:14]\n",
    "    #fig = plt.figure()\n",
    "\n",
    "    \n",
    "    for it in tqdm(range(args.n_cv_iter)):\n",
    "        #try:\n",
    "        #    train_x, train_y = next(data_iter)\n",
    "        #except:\n",
    "        #    data_iter = iter(train_dl)\n",
    "        #    train_x, train_y = next(data_iter)\n",
    "        #neural_control_variate.train_x = train_x\n",
    "        #neural_control_variate.train_y = train_y\n",
    "        ncv_optimizer.zero_grad()\n",
    "        mc_variance, no_cv_variance = compute_naive_variance(function_f, neural_control_variate, models, x, ll_div)\n",
    "        history.append(mc_variance.mean().item())\n",
    "        (mc_variance + 1e-3*centr_regularizer(neural_control_variate, models, x, ll_div)).mean().backward()\n",
    "        ncv_optimizer.step()\n",
    "    ncv_s.append(neural_control_variate)\n",
    "    print(mc_variance.mean().item(), no_cv_variance.mean().item())\n",
    "    # plt.plot(np.arange(it+1), history)\n",
    "    # plt.axhline(y=no_cv_variance.mean(), color='r', linestyle='-')\n",
    "    # plt.xlim(0, args.n_cv_iter)\n",
    "    # plt.ylim(0, 0.02)\n",
    "    # plt.show()\n",
    "    print(neural_control_variate(models, x).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_weights = [deepcopy(ncv.psy_model.state_dict()) for ncv in ncv_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0a7f69f95090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpsy_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "psy_weights[0], priors[0], trajectories[0][0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6f9caabe6d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSteinCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsy_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "cv = SteinCV(psy_model, train_x, train_y, priors[0], N_train)\n",
    "cv.psy_model.load_state_dict(psy_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0., grad_fn=<MeanBackward0>)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "cv(trajectories[0], x_new[13:14]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-fa6c871b9027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mncv_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ncv_s[0](trajectories[0], x_new[13:14]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(2.8215)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "x_new[13:14].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/psy_weights.pckl', 'wb') as fp:\n",
    "    pickle.dump(psy_weights, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/psy_weights.pckl', 'rb') as fp:\n",
    "    psy_weights = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_variates.cv_utils import trapezoidal_kernel, SpectralVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n ), LogRegression(\n   (linear): Linear(in_features=784, out_features=2, bias=True)\n )]"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  4%|▍         | 4/100 [00:19<07:37,  4.76s/it]"
    }
   ],
   "source": [
    "ncv_s2 = []\n",
    "for models, pr in zip(trajectories, priors):\n",
    "    psy_model = PsyConstVector(psy_input_dim)\n",
    "    psy_model.init_zero()\n",
    "    psy_model.to(device)\n",
    "\n",
    "    neural_control_variate = SteinCV(psy_model, train_x, train_y, priors, N_train)\n",
    "    function_f = lambda model, x: get_binary_prediction(model, x, classes=[0, 1])\n",
    "\n",
    "    function_h = lambda model, x: function_f(model, x) - neural_control_variate(model, x)\n",
    "\n",
    "    window_lag_f = trapezoidal_kernel\n",
    "    truncation_point = 40 #len(models) \n",
    "    spectral_loss = SpectralVariance(function_h, models, window_lag_f, truncation_point)\n",
    "    no_cv_loss = SpectralVariance(function_f, models, window_lag_f, truncation_point)\n",
    "\n",
    "    ncv_optimizer = torch.optim.Adam(psy_model.parameters(), lr=args.cv_lr, weight_decay=0.0) #1e-4)\n",
    "\n",
    "    history = [] \n",
    "    x = x_new[13:14]\n",
    "    #fig = plt.figure()\n",
    "\n",
    "    no_cv_variance = no_cv_loss(x)\n",
    "\n",
    "    data_iter = iter(train_dl)\n",
    "    for it in tqdm(range(args.n_cv_iter)):\n",
    "        try:\n",
    "            train_x, train_y = next(data_iter)\n",
    "        except:\n",
    "            data_iter = iter(train_dl)\n",
    "            train_x, train_y = next(data_iter)\n",
    "        neural_control_variate.train_x = train_x\n",
    "        neural_control_variate.train_y = train_y\n",
    "        ncv_optimizer.zero_grad()\n",
    "        mc_variance = spectral_loss(x)\n",
    "        history.append(mc_variance.mean().item())\n",
    "        (mc_variance + 1e-4*centr_regularizer(neural_control_variate, models, x)).backward()\n",
    "        ncv_optimizer.step()\n",
    "    ncv_s2.append(neural_control_variate)\n",
    "    print(mc_variance.mean().item(), no_cv_variance.mean().item())\n",
    "    print(neural_control_variate(models, x).mean())\n",
    "    # plt.plot(np.arange(it+1), history)\n",
    "    # plt.axhline(y=no_cv_variance.mean(), color='r', linestyle='-')\n",
    "    # plt.xlim(0, args.n_cv_iter)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "psy_weights2 = [deepcopy(ncv.psy_model.state_dict()) for ncv in ncv_s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/psy_weights2.pckl', 'wb') as fp:\n",
    "    pickle.dump(psy_weights2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}